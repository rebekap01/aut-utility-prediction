{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Different Embedding Sets on AUT Response Utility Prediction\n",
        "\n",
        "This notebook serves to calculate a range of embeddings (USE, GloVe, BERT, ELMo, Word2Vec, FastText) for a dataset of responses to the Alternate Uses Task. Then, non-generative machine learning models are trained using a single set of embeddings as predictors. The performance of the models is compared to establish which set of embeddings is most useful in predicting human ratings of utility on AUT response datasets."
      ],
      "metadata": {
        "id": "ohRumh2-RSJ0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kZsYfDEnu1v"
      },
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Packages\n",
        "\n",
        "Before running this make sure the utils.py document is downloaded from [here](https://github.com/allenai/comet-atomic-2020/tree/master/models/comet_atomic2020_bart)."
      ],
      "metadata": {
        "id": "cxCvPFKlJ86p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaV2IynAo7dW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "import requests\n",
        "import zipfile\n",
        "import time\n",
        "import seaborn as sns\n",
        "import chardet\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "from scipy import spatial\n",
        "from scipy.stats import pearsonr\n",
        "from utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder, RobustScaler, LabelEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BertTokenizer, BertModel\n",
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "W8i7Dkl7J5eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Data"
      ],
      "metadata": {
        "id": "NJfYDuO2J_0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVTM1_ajo_uR"
      },
      "outputs": [],
      "source": [
        "# Function for reading in Stevenson, 2020 data files and uniting rater_01 and rater_02 files\n",
        "def read_files(path):\n",
        "    \"\"\"\n",
        "    Read the csv files from ./data/Stevenson-2020-human\n",
        "\n",
        "    :param path: string with path to files\n",
        "    :return dataset: merged dataset\n",
        "    \"\"\"\n",
        "\n",
        "    path = path\n",
        "    all_files = glob.glob(path + \"/*.csv\")\n",
        "    liR1 = []\n",
        "    liR2 = []\n",
        "\n",
        "    for filename in all_files:\n",
        "        df = pd.read_csv(filename, index_col=None, header=0, nrows=1, encoding='latin1')\n",
        "        if len(df.columns) == 1:\n",
        "            df = pd.read_csv(filename, index_col=None, header=0, encoding='latin1', sep=';')\n",
        "        else:\n",
        "            df = pd.read_csv(filename, index_col=None, header=0, encoding='latin1')\n",
        "\n",
        "        if '_rater01' in filename:\n",
        "            liR1.append(df)\n",
        "\n",
        "\n",
        "        else:\n",
        "            liR2.append(df.loc[:, ['response_id', 'respondent_id', 'originality_rater02', 'utility_rater02']])\n",
        "\n",
        "    frameR1 = pd.concat(liR1, axis=0, ignore_index=True)\n",
        "    frameR2 = pd.concat(liR2, axis=0, ignore_index=True)\n",
        "\n",
        "    df = frameR1.merge(frameR2, on=['response_id','respondent_id'],\n",
        "                   how='left')\n",
        "\n",
        "    df[\"translated_response\"] = df[\"translated_response\"].astype(str)\n",
        "    df[\"response_id\"] = df[\"response_id\"].astype(str)\n",
        "    df[\"respondent_id\"] = df[\"respondent_id\"].astype(str)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in files where encoding needs to be detected (change file paths as needed)\n",
        "with open('/data/Nath-2024-LLM.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "df1 = pd.read_csv('/data/Nath-2024-LLM.csv', encoding=result['encoding'])\n",
        "\n",
        "with open('/data/Hubert-2024-LLM.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "df2 = pd.read_csv('/data/Hubert-2024-LLM.csv', encoding=result['encoding'])"
      ],
      "metadata": {
        "id": "90WAwjE2KJ4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rMDxpfRpDUC"
      },
      "outputs": [],
      "source": [
        "# Loading the rest of the data files (change file paths as needed)\n",
        "df3 = pd.read_csv('/data/Nath-2024-human.csv')\n",
        "df4 = pd.read_csv('/data/Stevenson-2022-human.csv')\n",
        "df5 = read_files('/data/Stevenson-2020-human')\n",
        "df6 = pd.read_excel('/data/additional-LLM.xlsx')\n",
        "\n",
        "# Uniting all data files in one dataframe\n",
        "df = pd.concat([df1, df2, df3, df4, df5, df6], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGBTzItpJu_"
      },
      "source": [
        "## Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGAPUA4RpMHn"
      },
      "outputs": [],
      "source": [
        "# Function for dropping invalid answers\n",
        "def drop_invalid(df):\n",
        "    \"\"\"\n",
        "    Drops all answers that were either empty, had a rating of 0 for at least one score,\n",
        "    or of which the respondent number was 9999 (indicating an invalid respondent)\n",
        "\n",
        "    :param df: dataset with all columns needed for further steps\n",
        "    :return dataset, dropped_data:  dataset without invalid data,\n",
        "                dataset of invalid data\n",
        "    \"\"\"\n",
        "    liV = [1] * len(df)\n",
        "    condition = (df[['utility_rater01', 'utility_rater02', 'originality_rater01', 'originality_rater02']] == 0).any(axis=1)\n",
        "\n",
        "    liV = [0 if cond else li for cond, li in zip(condition, liV)]\n",
        "\n",
        "    # Dropping answers rated as 0 by at least one rater\n",
        "    df['valid'] = liV\n",
        "    df_invalid = df[df['valid'] == 0]\n",
        "    df = df[df['valid'] != 0]\n",
        "\n",
        "    # Dropping respondent_id that seems to belong to no one\n",
        "    df_strange = df[df['respondent_id'] == 9999]\n",
        "    df = df[df['respondent_id'] != 9999]\n",
        "\n",
        "    # Dropping empty answers\n",
        "    df_empty = df[df['original_response'] == 'nan']\n",
        "    df = df[df['original_response'] != 'nan']\n",
        "    df = df.drop(columns=['valid'])\n",
        "\n",
        "    df_dropped = pd.concat([df_empty, df_strange, df_invalid], axis=0, ignore_index=True)\n",
        "\n",
        "    return df, df_dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wiXRTkapP09"
      },
      "outputs": [],
      "source": [
        "# Applying to dataset\n",
        "df, df_dropped = drop_invalid(df)\n",
        "num_dropped = len(df_dropped)\n",
        "\n",
        "print(num_dropped)\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWl-7uyrpSkz"
      },
      "outputs": [],
      "source": [
        "# Function for cleaning valid responses\n",
        "def clean_response(dataset, col_response):\n",
        "    \"\"\"\n",
        "    Function cleans the responses\n",
        "\n",
        "    :param dataset: dataset which include column(s) of responses\n",
        "    :param col_response: column name of responses to be cleaned\n",
        "    :return dataset: input dataset with clean responses added\n",
        "    \"\"\"\n",
        "    # Upper to lowercase, remove punctuation and redundant spaces/letters\n",
        "    dataset[col_response] = [x.lower() for x in dataset[col_response]]\n",
        "    dataset[col_response] = [re.sub(r'[^\\w\\s]', ' ', x) for x in dataset[col_response]]  # delete any signs\n",
        "    dataset[col_response] = [re.sub(r'\\b\\w\\b', ' ', x) for x in dataset[col_response]] # delete loose letters\n",
        "    dataset[col_response] = [x.strip() for x in dataset[col_response]]  # delete extra white space before/after string\n",
        "    dataset[col_response] = [' '.join(x.split()) for x in dataset[col_response]]  # delete every extra space in string\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zcxJF-IpVvp"
      },
      "outputs": [],
      "source": [
        "# Applying to dataset\n",
        "df = clean_response(df, 'translated_response')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming utility from 5-point scale to three categories\n",
        "df = df.dropna(subset=[\"originality_rater01\", \"utility_rater01\"])\n",
        "\n",
        "# Function to categorize ratings\n",
        "def categorize_rating(rating):\n",
        "    if rating in [1, 2]:\n",
        "        return 'low'\n",
        "    elif rating in [3, 4]:\n",
        "        return 'medium'\n",
        "    elif rating == 5:\n",
        "        return 'high'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "df['category_rater01'] = df['utility_rater01'].apply(categorize_rating)\n",
        "df['category_rater02'] = df['utility_rater02'].apply(lambda x: categorize_rating(x) if not pd.isna(x) else np.nan)\n",
        "\n",
        "# Function that takes rater 1's (better rater's) scores if raters disagree on category\n",
        "def determine_final_category(row):\n",
        "    if pd.isna(row['utility_rater02']):\n",
        "        return row['category_rater01']\n",
        "    elif row['category_rater01'] == row['category_rater02']:\n",
        "        return row['category_rater01']\n",
        "    else:\n",
        "        return row['category_rater02']\n",
        "\n",
        "df['final_category'] = df.apply(determine_final_category, axis=1)\n",
        "\n",
        "# Drop intermediate categories\n",
        "df = df.drop(columns=['category_rater01', 'category_rater02'])"
      ],
      "metadata": {
        "id": "U24UrKQwXmx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzKD4nE3lZIc"
      },
      "source": [
        "## Universal Sentence Encoder (USE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slm9irPSlNaM"
      },
      "outputs": [],
      "source": [
        "# Getting Universal Sentence Encoder embeddings\n",
        "def get_embeddings(texts, batch_size=100):\n",
        "      # Load USE module\n",
        "      os.environ['TFHUB_CACHE_DIR'] = '/tf_cache'\n",
        "      module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3')\n",
        "      embeddings = []\n",
        "      for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        embeddings.append(module(batch).numpy())\n",
        "\n",
        "      return np.vstack(embeddings)\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "      return 1 - spatial.distance.cosine(vec1, vec2)\n",
        "\n",
        "# Function for calculating semantic distance from USE embeddings\n",
        "def sem_dis(dataset, batch_size=100):\n",
        "    \"\"\"\n",
        "    Function to add word embeddings and calculate semantic distance.\n",
        "\n",
        "    :param dataset: dataset containing at least the responses\n",
        "    :return dataset: input with extra columns for the word embedding values (one column per value)\n",
        "                        and the semantic distance\n",
        "    \"\"\"\n",
        "    # Array containing all original responses\n",
        "    responses = dataset[['translated_response']].copy()\n",
        "    objs = pd.DataFrame([['belt'], ['book'], ['brick'], ['can'], ['fork'], ['paperclip'], ['stick'], ['towel']], columns=['translated_response'])\n",
        "    responses = pd.concat([responses, objs], axis=0, ignore_index=True)\n",
        "    responses = responses.to_numpy()\n",
        "\n",
        "    # Get embeddings for top 10 uses for each object\n",
        "    top_use_embeddings = {obj: get_embeddings(uses) for obj, uses in top_10s.items()}\n",
        "\n",
        "    # Get embeddings for the 'translated_response' column in the dataset\n",
        "    emb = get_embeddings(responses, batch_size=batch_size)\n",
        "    emb = pd.DataFrame(emb)\n",
        "    emb.columns = emb.columns.astype(str)\n",
        "\n",
        "    # Word embeddings of the AUT objects\n",
        "    belt = emb.iloc[-8, :]\n",
        "    book = emb.iloc[-7, :]\n",
        "    brick = emb.iloc[-6, :]\n",
        "    can = emb.iloc[-5, :]\n",
        "    fork = emb.iloc[-4, :]\n",
        "    paperclip = emb.iloc[-3, :]\n",
        "    stick = emb.iloc[-2, :]\n",
        "    towel = emb.iloc[-1, :]\n",
        "\n",
        "    emb = emb.iloc[:-8, :]\n",
        "    dataset = pd.concat([dataset.reset_index(drop=True), emb], axis=1)\n",
        "\n",
        "    dist = []\n",
        "\n",
        "    # Calculate semantic distance from AUT object for each response\n",
        "    for i in range(len(emb)):\n",
        "        if dataset['object'][i] == 'belt':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], belt))\n",
        "        elif dataset['object'][i] == 'book':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], book))\n",
        "        elif dataset['object'][i] == 'brick':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], brick))\n",
        "        elif dataset['object'][i] == 'can':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], can))\n",
        "        elif dataset['object'][i] == 'fork':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], fork))\n",
        "        elif dataset['object'][i] == 'paperclip':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], paperclip))\n",
        "        elif dataset['object'][i] == 'stick':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], stick))\n",
        "        elif dataset['object'][i] == 'towel':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], towel))\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "      obj = row['object']\n",
        "      use_embedding = emb.iloc[i, :]\n",
        "\n",
        "      # Calculate similarity between response and top 10 uses for that object\n",
        "      if obj in top_use_embeddings:\n",
        "          similarities = [cosine_similarity(use_embedding, top_use_embedding) for top_use_embedding in top_use_embeddings[obj]]\n",
        "          # Store similarity to each of the top 10 uses in a separate column\n",
        "          for j, sim in enumerate(similarities):\n",
        "              dataset.loc[i, f'sim_{j+1}'] = sim\n",
        "\n",
        "    dataset[\"similarity\"] = dist\n",
        "\n",
        "    return dataset, emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT2BfaTalnpR"
      },
      "outputs": [],
      "source": [
        "# Adding semantic distance to df + creating embeddings\n",
        "df, embeddings = sem_dis(df, batch_size=100)\n",
        "\n",
        "# Descriptive statistics of semantic distance\n",
        "mean_sem_dis = df.similarity.mean().round(2)\n",
        "sd_sem_dis = df.similarity.std().round(2)\n",
        "print(mean_sem_dis)\n",
        "print(sd_sem_dis)\n",
        "\n",
        "max_sem_dis = df.similarity.max().round(2)\n",
        "max_sem_dis_resp = df['translated_response'][df.similarity.idxmax()]\n",
        "max_sem_dis_obj = df['object'][df.similarity.idxmax()]\n",
        "print(max_sem_dis)\n",
        "print(max_sem_dis_resp)\n",
        "print(max_sem_dis_obj)\n",
        "\n",
        "min_sem_dis = df.similarity.min().round(2)\n",
        "min_sem_dis_resp = df['translated_response'][df.similarity.idxmin()]\n",
        "min_sem_dis_obj = df['object'][df.similarity.idxmin()]\n",
        "print(min_sem_dis)\n",
        "print(min_sem_dis_resp)\n",
        "print(min_sem_dis_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKUMi5BsljhD"
      },
      "source": [
        "## GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saE4UQslmAPG"
      },
      "outputs": [],
      "source": [
        "# Downloading GloVe embeddings\n",
        "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "glove_zip = \"glove.6B.zip\"\n",
        "glove_dir = \"glove.6B\"\n",
        "\n",
        "if not os.path.exists(glove_zip):\n",
        "    print(f\"Downloading {glove_zip}...\")\n",
        "    response = requests.get(glove_url)\n",
        "    with open(glove_zip, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Extracting the embeddings\n",
        "if not os.path.exists(glove_dir):\n",
        "    print(f\"Extracting {glove_zip}...\")\n",
        "    with zipfile.ZipFile(glove_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(glove_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUvtaqY5mBtv"
      },
      "outputs": [],
      "source": [
        "# Function for loading the GloVe embeddings\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
        "    return embeddings_index\n",
        "\n",
        "glove_file = \"glove.6B/glove.6B.100d.txt\"\n",
        "embeddings_index = load_glove_embeddings(glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmtxUw83mJHr"
      },
      "outputs": [],
      "source": [
        "# Function for getting the GloVe embedding for each response\n",
        "def get_glove_embeddings(df, embeddings_index, embedding_dim=100):\n",
        "    texts = df['translated_response'].to_numpy()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    embeddings = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "        word_embeddings = [embeddings_index[word] for word in words if word in embeddings_index]\n",
        "\n",
        "        if word_embeddings:\n",
        "            sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            sentence_embedding = np.zeros(embedding_dim)\n",
        "\n",
        "        embeddings.append(sentence_embedding)\n",
        "\n",
        "    # Converting embeddings from list to df\n",
        "    embeddings_df = pd.DataFrame(embeddings, columns=[f\"embedding_{i}\" for i in range(embedding_dim)])\n",
        "\n",
        "    # Adding to the original df\n",
        "    df = pd.concat([df.reset_index(drop=True), embeddings_df], axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqE5LvcOmLNQ"
      },
      "outputs": [],
      "source": [
        "# Applying to the dataset\n",
        "df = get_glove_embeddings(df, embeddings_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D61VWAKdmibS"
      },
      "source": [
        "## BERT Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZD1xDSLmlS1"
      },
      "outputs": [],
      "source": [
        "# Initializing BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Function for getting the BERT embedding for each response\n",
        "def get_bert_embeddings(df, embedding_dim=768):\n",
        "    texts = df['translated_response'].tolist()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    embeddings = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "        # Tokenizing text\n",
        "        inputs = tokenizer(\" \".join(words), return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "        inputs.to(device)\n",
        "\n",
        "        # Getting BERT embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Average across all token embeddings to get sentence embedding\n",
        "        token_embeddings = outputs.last_hidden_state\n",
        "        sentence_embedding = torch.mean(token_embeddings, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "        embeddings.append(sentence_embedding)\n",
        "\n",
        "    # Converting embeddings from list to df\n",
        "    embeddings_df = pd.DataFrame(embeddings, columns=[f\"bert_embedding_{i}\" for i in range(embedding_dim)])\n",
        "\n",
        "    # Adding to the original df\n",
        "    df = pd.concat([df.reset_index(drop=True), embeddings_df], axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMI5nXDPmpis"
      },
      "outputs": [],
      "source": [
        "# Applying to the dataset\n",
        "df = get_bert_embeddings(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBMG2GY4mulU"
      },
      "source": [
        "## ELMo Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-V6fSEzm04t"
      },
      "outputs": [],
      "source": [
        "# Importing necessary files for ELMo\n",
        "options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "# Initializing embedder\n",
        "elmo = Elmo(options_file, weight_file, 1, dropout=0)\n",
        "\n",
        "# Function for getting the ELMo embedding for each response\n",
        "def get_elmo_embeddings(df, embedding_dim=1024, batch_size=32):\n",
        "    texts = df['translated_response'].tolist()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    embeddings = []\n",
        "\n",
        "    # Process responses in batches\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        batch_embeddings = []\n",
        "\n",
        "        for text in batch_texts:\n",
        "            words = word_tokenize(text)\n",
        "            words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "            if words:\n",
        "                # Getting ELMo embeddings\n",
        "                character_ids = batch_to_ids([words])\n",
        "                with torch.no_grad():\n",
        "                    elmo_embeddings = elmo(character_ids)['elmo_representations'][0].numpy()\n",
        "\n",
        "                # Average across all token embeddings to get sentence embedding\n",
        "                sentence_embedding = np.mean(elmo_embeddings, axis=1).squeeze()\n",
        "            else:\n",
        "                sentence_embedding = np.zeros(embedding_dim)\n",
        "\n",
        "            batch_embeddings.append(sentence_embedding)\n",
        "\n",
        "        embeddings.extend(batch_embeddings)\n",
        "\n",
        "    # Converting embeddings from list to df\n",
        "    embeddings_df = pd.DataFrame(embeddings, columns=[f\"embedding_{i}\" for i in range(embedding_dim)])\n",
        "\n",
        "    # Adding to the original df\n",
        "    df = pd.concat([df.reset_index(drop=True), embeddings_df], axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qybjc3nCm5Kv"
      },
      "outputs": [],
      "source": [
        "# Applying to the dataset\n",
        "df = get_elmo_embeddings(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHN160YWnQlU"
      },
      "source": [
        "## Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEutUZh-nTov"
      },
      "outputs": [],
      "source": [
        "# Loading Word2Vec model\n",
        "model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Function for getting the Word2Vec embedding for each response\n",
        "def get_word2vec_embeddings(df, embedding_dim=300):\n",
        "    texts = df['translated_response'].tolist()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    embeddings = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "        # Getting Word2Vec embeddings\n",
        "        word_embeddings = [model[word] for word in words if word in model]\n",
        "\n",
        "        # Average across all token embeddings to get sentence embedding\n",
        "        if word_embeddings:\n",
        "            sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            sentence_embedding = np.zeros(embedding_dim)\n",
        "\n",
        "        embeddings.append(sentence_embedding)\n",
        "\n",
        "    # Converting embeddings from list to df\n",
        "    embeddings_df = pd.DataFrame(embeddings, columns=[f\"embedding_{i}\" for i in range(embedding_dim)])\n",
        "\n",
        "    # Adding to the original df\n",
        "    df = pd.concat([df.reset_index(drop=True), embeddings_df], axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying to the dataset\n",
        "df = get_word2vec_embeddings(df)"
      ],
      "metadata": {
        "id": "Lbm3kYx-OxRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaLGJKlXnex6"
      },
      "source": [
        "## FastText Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1xMBYv-nnX_"
      },
      "outputs": [],
      "source": [
        "# Loading FastText model\n",
        "fasttext.util.download_model('en', if_exists='ignore')\n",
        "model = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "# Function for getting the FastText embedding for each response\n",
        "def get_fasttext_embeddings(df, embedding_dim=300):\n",
        "    texts = df['translated_response'].tolist()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    embeddings = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "        # Getting FastText embeddings\n",
        "        word_embeddings = [model.get_word_vector(word) for word in words if word in model.words]\n",
        "\n",
        "        # Average across all token embeddings to get sentence embedding\n",
        "        if word_embeddings:\n",
        "            sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            sentence_embedding = np.zeros(embedding_dim)\n",
        "\n",
        "        embeddings.append(sentence_embedding)\n",
        "\n",
        "    # Converting embeddings from list to df\n",
        "    embeddings_df = pd.DataFrame(embeddings, columns=[f\"embedding_{i}\" for i in range(embedding_dim)])\n",
        "\n",
        "    # Adding to the original df\n",
        "    df = pd.concat([df.reset_index(drop=True), embeddings_df], axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying to the dataset\n",
        "df = get_fasttext_embeddings(df)"
      ],
      "metadata": {
        "id": "Dj_vgdjqPdH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmu9CjVUp24k"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating training + validation and test sets\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Separate out the 'towel' responses (which is the hold-out AUT object)\n",
        "towel_responses = df[df['object'] == 'towel']\n",
        "non_towel_responses = df[df['object'] != 'towel']\n",
        "\n",
        "# Perform stratified sampling on the non-towel responses\n",
        "train_non_towel, test_non_towel = train_test_split(\n",
        "    non_towel_responses,\n",
        "    test_size=0.1,\n",
        "    stratify=non_towel_responses['final_category'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Form the final test set ('towel' responses + stratified sample of remaining)\n",
        "test_set = pd.concat([towel_responses, test_non_towel])\n",
        "\n",
        "# Form the final training + validation set\n",
        "train_set = non_towel_responses[~non_towel_responses.index.isin(test_non_towel.index)]\n",
        "train_set = pd.concat([train_set, train_non_towel])\n",
        "\n",
        "# Shuffle final training and test sets\n",
        "train_set = train_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_set = test_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Drop duplicates based on 'translated_response', keeping the first occurrence\n",
        "train_set = train_set.groupby('object', group_keys=False).apply(lambda x: x.drop_duplicates(subset='translated_response', keep='first'))\n",
        "\n",
        "# Reset the index\n",
        "train_set = train_set.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "8YABYwS6Zi7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up for training Naive Bayes, Logistic Regression, k-nearest Neighbors and LightGBM models\n",
        "\n",
        "def get_feature_set(dataset):\n",
        "    # Adjust based on which embedding set you want to train models on\n",
        "    i1 = dataset.columns.get_loc('final_category') + 1\n",
        "    i2 = dataset.columns.get_loc('511') + 1\n",
        "    feature_set = dataset.iloc[:, i1:i2]\n",
        "\n",
        "# Function for calculating AIC and BIC for model comparison\n",
        "def calculate_aic_bic(log_likelihood, n_params, n_samples):\n",
        "    aic = 2 * n_params - 2 * log_likelihood\n",
        "    bic = np.log(n_samples) * n_params - 2 * log_likelihood\n",
        "    return aic, bic\n",
        "\n",
        "def train_models(train_set, test_set, print_results=False):\n",
        "    SEED = 1\n",
        "    i = 0\n",
        "\n",
        "    # Transforming categorical labels into numbers for model\n",
        "    le = LabelEncoder()\n",
        "    train_set['final_category_encoded'] = le.fit_transform(train_set['final_category'])\n",
        "    test_set['final_category_encoded'] = le.transform(test_set['final_category'])\n",
        "\n",
        "    # Getting feature set and outcome variable\n",
        "    predictorVar = get_feature_set(train_set)\n",
        "    targetVar = 'final_category_encoded'\n",
        "    X_trainval = train_set[predictorVar.columns]\n",
        "    y_trainval = train_set[targetVar]\n",
        "    X_test = test_set[predictorVar.columns]\n",
        "    y_test = test_set[targetVar]\n",
        "\n",
        "    # Setting up 5-fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setting up hyperparameters for each model (adjust as needed)\n",
        "    params_log_reg = {\n",
        "        'C': 1.0,\n",
        "        'penalty': 'l2',\n",
        "        'solver': 'liblinear',\n",
        "        'random_state': SEED\n",
        "    }\n",
        "\n",
        "    params_knn = {\n",
        "        'n_neighbors': 5,\n",
        "        'weights': 'uniform',\n",
        "        'algorithm': 'auto'\n",
        "    }\n",
        "\n",
        "    params_lgb = {\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 3,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.1,\n",
        "        'n_estimators': 100,\n",
        "        'random_state': SEED\n",
        "    }\n",
        "\n",
        "    log_reg = LogisticRegression(**params_log_reg)\n",
        "    nb = GaussianNB()\n",
        "    knn = KNeighborsClassifier(**params_knn)\n",
        "    lgb_model = lgb.LGBMClassifier(**params_lgb)\n",
        "\n",
        "    # Set up saving for trained models\n",
        "    trained_models = {\"utility\": dict()}\n",
        "    df_predictions = pd.DataFrame(columns=['measure', 'model', 'set', 'actual', 'predictions', 'accuracy', 'precision', 'recall', 'f1_score', 'aic', 'bic'], index=range(0, 14))\n",
        "\n",
        "    models = [('Logistic Regression', log_reg), ('Naive Bayes', nb), ('KNN', knn), ('LightGBM', lgb_model), ('Base Model', 'mode')]\n",
        "\n",
        "    accuracy_test_scores = []\n",
        "    accuracy_val_scores = []\n",
        "    precision_test_scores = []\n",
        "    precision_val_scores = []\n",
        "    recall_test_scores = []\n",
        "    recall_val_scores = []\n",
        "    f1_test_scores = []\n",
        "    f1_val_scores = []\n",
        "    aic_test_scores = []\n",
        "    aic_val_scores = []\n",
        "    bic_test_scores = []\n",
        "    bic_val_scores = []\n",
        "\n",
        "   # Training and validating each model using 5-fold cross-validation\n",
        "    for ml_name, ml in models:\n",
        "        for train_index, val_index in skf.split(X_trainval, y_trainval):\n",
        "            X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
        "            y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
        "\n",
        "            if ml_name == 'Base Model':\n",
        "                mode_val = train_set['final_category_encoded'].mode()[0]\n",
        "                y_pred_val = np.repeat(mode_val, len(X_val))\n",
        "                y_pred_test = np.repeat(mode_val, len(X_test))\n",
        "                y_pred_prob_val = np.ones((len(X_val), len(le.classes_))) / len(le.classes_)\n",
        "                y_pred_prob_test = np.ones((len(X_test), len(le.classes_))) / len(le.classes_)\n",
        "            else:\n",
        "                ml.fit(X_train, y_train)\n",
        "                y_pred_val = ml.predict(X_val)\n",
        "                y_pred_test = ml.predict(X_test)\n",
        "                y_pred_prob_val = ml.predict_proba(X_val)\n",
        "                y_pred_prob_test = ml.predict_proba(X_test)\n",
        "\n",
        "            trained_models['utility'].update({ml_name: ml})\n",
        "\n",
        "            # Calculating accuracy, precision, recall and F1-score\n",
        "            accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "            precision_val = precision_score(y_val, y_pred_val, average='weighted', zero_division=0)\n",
        "            recall_val = recall_score(y_val, y_pred_val, average='weighted')\n",
        "            f1_val = f1_score(y_val, y_pred_val, average='weighted')\n",
        "\n",
        "            accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "            precision_test = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "            recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "            f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "            # Calculating AIC and BIC\n",
        "            log_likelihood_val = -log_loss(y_val, y_pred_prob_val, labels=np.arange(len(le.classes_)))\n",
        "            log_likelihood_test = -log_loss(y_test, y_pred_prob_test, labels=np.arange(len(le.classes_)))\n",
        "            n_params = len(ml.get_params()) if ml_name != 'Base Model' else 0\n",
        "            n_samples_val = len(y_val)\n",
        "            n_samples_test = len(y_test)\n",
        "            aic_val, bic_val = calculate_aic_bic(log_likelihood_val, n_params, n_samples_val)\n",
        "            aic_test, bic_test = calculate_aic_bic(log_likelihood_test, n_params, n_samples_test)\n",
        "\n",
        "            # Appending metrics to appropriate lists\n",
        "            accuracy_test_scores.extend([accuracy_test])\n",
        "            accuracy_val_scores.extend([accuracy_val])\n",
        "            precision_test_scores.extend([precision_test])\n",
        "            precision_val_scores.extend([precision_val])\n",
        "            recall_test_scores.extend([recall_test])\n",
        "            recall_val_scores.extend([recall_val])\n",
        "            f1_test_scores.extend([f1_test])\n",
        "            f1_val_scores.extend([f1_val])\n",
        "            aic_test_scores.extend([aic_test])\n",
        "            aic_val_scores.extend([aic_val])\n",
        "            bic_test_scores.extend([bic_test])\n",
        "            bic_val_scores.extend([bic_val])\n",
        "\n",
        "            # Adding results to df_predictions\n",
        "            df_predictions.loc[i] = np.array(['utility', ml_name, 'test', y_test, y_pred_test, accuracy_test, precision_test, recall_test, f1_test,\n",
        "                                             aic_test, bic_test], dtype=object)\n",
        "\n",
        "            df_predictions.loc[i + 1] = np.array(['utility', ml_name, 'validation', y_val, y_pred_val, accuracy_val, precision_val, recall_val, f1_val,\n",
        "                                                 aic_val, bic_val], dtype=object)\n",
        "\n",
        "            i += 2\n",
        "\n",
        "        # Calculating mean metrics across folds\n",
        "        mean_accuracy_test = np.mean(accuracy_test_scores)\n",
        "        mean_accuracy_val = np.mean(accuracy_val_scores)\n",
        "        mean_precision_test = np.mean(precision_test_scores)\n",
        "        mean_precision_val = np.mean(precision_val_scores)\n",
        "        mean_recall_test = np.mean(recall_test_scores)\n",
        "        mean_recall_val = np.mean(recall_val_scores)\n",
        "        mean_f1_test = np.mean(f1_test_scores)\n",
        "        mean_f1_val = np.mean(f1_val_scores)\n",
        "        mean_aic_test = np.mean(aic_test_scores)\n",
        "        mean_aic_val = np.mean(aic_val_scores)\n",
        "        mean_bic_test = np.mean(bic_test_scores)\n",
        "        mean_bic_val = np.mean(bic_val_scores)\n",
        "\n",
        "        # Printing metrics\n",
        "        if print_results:\n",
        "            print(y_pred_test)\n",
        "            print(\"utility \\n\")\n",
        "            print(\"{:s} \\n Accuracy: {:.3f}, \\n Precision: {:.3f}, \\n Recall: {:.3f} \\n F1: {:.3f}, AIC: {:.3f}, BIC: {:.3f} \\n\".format(ml_name, mean_accuracy_test, mean_precision_test, mean_recall_test, mean_f1_test, mean_aic_test, mean_bic_test))\n",
        "            print(\"\\n\")\n",
        "            print(\"{:s} \\n Accuracy: {:.3f}, \\n Precision: {:.3f}, \\n Recall: {:.3f} \\n F1: {:.3f}, AIC: {:.3f}, BIC: {:.3f} \\n\".format('test set', mean_accuracy_val, mean_precision_val, mean_recall_val, mean_f1_val, mean_aic_val, mean_bic_val))\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return df_predictions, trained_models"
      ],
      "metadata": {
        "id": "qc9H-RUfPuSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up for training random forest classifier and XGBoost models\n",
        "\n",
        "def get_feature_set(dataset):\n",
        "    # Adjust based on which embedding set you want to train models on\n",
        "    i1 = dataset.columns.get_loc('final_category') + 1\n",
        "    i2 = dataset.columns.get_loc('511') + 1\n",
        "    feature_set = dataset.iloc[:, i1:i2]\n",
        "    return feature_set\n",
        "\n",
        "# Function for calculating AIC and BIC for model comparison\n",
        "def calculate_aic_bic(log_likelihood, n_params, n_samples):\n",
        "    aic = 2 * n_params - 2 * log_likelihood\n",
        "    bic = np.log(n_samples) * n_params - 2 * log_likelihood\n",
        "    return aic, bic\n",
        "\n",
        "def train_models(train_set, test_set, print_results=False):\n",
        "    SEED = 1\n",
        "    i = 0\n",
        "\n",
        "    # Transforming categorical labels into numbers for model\n",
        "    le = LabelEncoder()\n",
        "    train_set['final_category_encoded'] = le.fit_transform(train_set['final_category'])\n",
        "    test_set['final_category_encoded'] = le.transform(test_set['final_category'])\n",
        "\n",
        "    # Getting feature set and outcome variable\n",
        "    predictorVar = get_feature_set(train_set)\n",
        "    targetVar = 'final_category_encoded'\n",
        "    X_trainval = train_set[predictorVar.columns]\n",
        "    y_trainval = train_set[targetVar]\n",
        "    X_test = test_set[predictorVar.columns]\n",
        "    y_test = test_set[targetVar]\n",
        "\n",
        "    # Setting up 5-fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setting up hyperparameters for each model (adjust as needed)\n",
        "    params_rf = {\n",
        "        'max_depth': 20,\n",
        "        'min_samples_leaf': 3,\n",
        "        'min_samples_split': 8,\n",
        "        'n_estimators': 200,\n",
        "        'class_weight': 'balanced'\n",
        "    }\n",
        "\n",
        "    params_xgb = {\n",
        "        'learning_rate': 0.2,\n",
        "        'max_depth': 10,\n",
        "        'n_estimators': 200,\n",
        "        'subsample': 0.6,\n",
        "        'eta': 0.1,\n",
        "        'objective': 'multi:softmax',  # for classification\n",
        "        'num_class': 3,\n",
        "        'scale_pos_weight': len(y_trainval) / (2 * np.bincount(y_trainval))\n",
        "    }\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=SEED)\n",
        "    rf.set_params(**params_rf)\n",
        "    xgb_model = xgb.XGBClassifier(**params_xgb)\n",
        "\n",
        "    # Set up saving for trained models\n",
        "    trained_models = {\"utility\": dict()}\n",
        "    df_predictions = pd.DataFrame(columns=['measure', 'model', 'set', 'actual', 'predictions', 'accuracy', 'precision', 'recall', 'f1_score', 'aic', 'bic'], index=range(0, 14))\n",
        "\n",
        "    models = [('Random Forest', rf), ('XGBoost', xgb_model), ('Base Model', 'mode')]\n",
        "    accuracy_test_scores = []\n",
        "    accuracy_val_scores = []\n",
        "    precision_test_scores = []\n",
        "    precision_val_scores = []\n",
        "    recall_test_scores = []\n",
        "    recall_val_scores = []\n",
        "    f1_test_scores = []\n",
        "    f1_val_scores = []\n",
        "    aic_test_scores = []\n",
        "    aic_val_scores = []\n",
        "    bic_test_scores = []\n",
        "    bic_val_scores = []\n",
        "\n",
        "    # Training and validating each model using 5-fold cross-validation\n",
        "    for ml_name, ml in models:\n",
        "      for train_index, val_index in skf.split(X_trainval, y_trainval):\n",
        "          X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
        "          y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
        "\n",
        "          if ml_name == 'Random Forest':\n",
        "              ml.fit(X_train, y_train)\n",
        "              y_pred_val = ml.predict(X_val)\n",
        "              y_pred_test = ml.predict(X_test)\n",
        "              y_pred_prob_val = ml.predict_proba(X_val)\n",
        "              y_pred_prob_test = ml.predict_proba(X_test)\n",
        "          elif ml_name == 'Base Model':\n",
        "              mode_val = train_set['final_category_encoded'].mode()[0]\n",
        "              y_pred_val = np.repeat(mode_val, len(X_val))\n",
        "              y_pred_test = np.repeat(mode_val, len(X_test))\n",
        "              y_pred_prob_val = np.ones((len(X_val), len(le.classes_))) / len(le.classes_)\n",
        "              y_pred_prob_test = np.ones((len(X_test), len(le.classes_))) / len(le.classes_)\n",
        "          else:\n",
        "              ml.fit(X_train, y_train)\n",
        "              y_pred_val = ml.predict(X_val)\n",
        "              y_pred_test = ml.predict(X_test)\n",
        "              y_pred_prob_val = ml.predict_proba(X_val)\n",
        "              y_pred_prob_test = ml.predict_proba(X_test)\n",
        "\n",
        "          trained_models['utility'].update({ml_name: ml})\n",
        "\n",
        "          # Calculating accuracy, precision, recall and F1-score\n",
        "          accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "          precision_val = precision_score(y_val, y_pred_val, average='weighted', zero_division=0)\n",
        "          recall_val = recall_score(y_val, y_pred_val, average='weighted')\n",
        "          f1_val = f1_score(y_val, y_pred_val, average='weighted')\n",
        "\n",
        "          accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "          precision_test = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "          recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "          f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "          # Calculating AIC and BIC\n",
        "          log_likelihood_val = -log_loss(y_val, y_pred_prob_val, labels=np.arange(len(le.classes_)))\n",
        "          log_likelihood_test = -log_loss(y_test, y_pred_prob_test, labels=np.arange(len(le.classes_)))\n",
        "          n_params_rf = len(params_rf) if ml_name == 'Random Forest' else len(params_xgb)\n",
        "          n_samples_val = len(y_val)\n",
        "          n_samples_test = len(y_test)\n",
        "          aic_val, bic_val = calculate_aic_bic(log_likelihood_val, n_params_rf, n_samples_val)\n",
        "          aic_test, bic_test = calculate_aic_bic(log_likelihood_test, n_params_rf, n_samples_test)\n",
        "\n",
        "          # Appending metrics to appropriate lists\n",
        "          accuracy_test_scores.extend([accuracy_test])\n",
        "          accuracy_val_scores.extend([accuracy_val])\n",
        "          precision_test_scores.extend([precision_test])\n",
        "          precision_val_scores.extend([precision_val])\n",
        "          recall_test_scores.extend([recall_test])\n",
        "          recall_val_scores.extend([recall_val])\n",
        "          f1_test_scores.extend([f1_test])\n",
        "          f1_val_scores.extend([f1_val])\n",
        "          aic_test_scores.extend([aic_test])\n",
        "          aic_val_scores.extend([aic_val])\n",
        "          bic_test_scores.extend([bic_test])\n",
        "          bic_val_scores.extend([bic_val])\n",
        "\n",
        "          # Adding results to df_predictions\n",
        "          df_predictions.loc[i] = np.array(['utility', ml_name, 'test', y_test, y_pred_test, accuracy_test, precision_test, recall_test, f1_test,\n",
        "                                             aic_test, bic_test], dtype=object)\n",
        "\n",
        "          df_predictions.loc[i + 1] = np.array(['utility', ml_name, 'validation', y_val, y_pred_val, accuracy_val, precision_val, recall_val, f1_val,\n",
        "                                                 aic_val, bic_val], dtype=object)\n",
        "\n",
        "          i += 2\n",
        "\n",
        "      # Calculating mean metrics across folds\n",
        "      mean_accuracy_test = np.mean(accuracy_test_scores)\n",
        "      mean_accuracy_val = np.mean(accuracy_val_scores)\n",
        "      mean_precision_test = np.mean(precision_test_scores)\n",
        "      mean_precision_val = np.mean(precision_val_scores)\n",
        "      mean_recall_test = np.mean(recall_test_scores)\n",
        "      mean_recall_val = np.mean(recall_val_scores)\n",
        "      mean_f1_test = np.mean(f1_test_scores)\n",
        "      mean_f1_val = np.mean(f1_val_scores)\n",
        "      mean_aic_test = np.mean(aic_test_scores)\n",
        "      mean_aic_val = np.mean(aic_val_scores)\n",
        "      mean_bic_test = np.mean(bic_test_scores)\n",
        "      mean_bic_val = np.mean(bic_val_scores)\n",
        "\n",
        "      # Printing metrics\n",
        "      if print_results:\n",
        "            print(y_pred_test)\n",
        "            print(\"utility \\n\")\n",
        "            print(\"{:s} \\n Accuracy: {:.3f}, \\n Precision: {:.3f}, \\n Recall: {:.3f} \\n F1: {:.3f}, AIC: {:.3f}, BIC: {:.3f} \\n\".format(ml_name, mean_accuracy_test, mean_precision_test, mean_recall_test, mean_f1_test, mean_aic_test, mean_bic_test))\n",
        "            print(\"\\n\")\n",
        "            print(\"{:s} \\n Accuracy: {:.3f}, \\n Precision: {:.3f}, \\n Recall: {:.3f} \\n F1: {:.3f}, AIC: {:.3f}, BIC: {:.3f} \\n\".format('test set', mean_accuracy_val, mean_precision_val, mean_recall_val, mean_f1_val, mean_aic_val, mean_bic_val))\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return df_predictions, trained_models"
      ],
      "metadata": {
        "id": "bvi9GhS3P8hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW0g_OR1qjiW"
      },
      "outputs": [],
      "source": [
        "# Train models\n",
        "models, predictions = train_models(train_set, test_set, print_results = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}