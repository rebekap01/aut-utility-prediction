{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Non-Generative Machine Learning Models to Predict Response Utility Ratings on the Alternate Uses Task\n",
        "\n",
        "The purpose of this notebook is to take data from a number of previous papers on the Alternate Uses Task (namely [Stevenson, 2020](http://modelingcreativity.org/blog/wp-content/uploads/2020/07/ABBAS_report_200711_final.pdf); [Stevenson, 2022](https://arxiv.org/pdf/2206.08932); [Nath, 2024](https://arxiv.org/pdf/2405.00899) and [Hubert, 2024](https://www.nature.com/articles/s41598-024-53303-w.pdf)), create several features that might be predictive of response utility and then train a range of non-generative machine learning models on this task to establish the best predictor-model combination for predicting human rater utility scores."
      ],
      "metadata": {
        "id": "CNRxsWInF64l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acuTtZdYVWx-"
      },
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LG-CiFB8paG"
      },
      "source": [
        "### Importing Packages\n",
        "\n",
        "Before running this make sure the utils.py document is downloaded from [here](https://github.com/allenai/comet-atomic-2020/tree/master/models/comet_atomic2020_bart)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FgJujttYX_s",
        "outputId": "efe80c77-0c72-4189-bd1e-010401145b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2086771beab5>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import zipfile\n",
        "import argparse\n",
        "import requests\n",
        "import time\n",
        "import seaborn as sns\n",
        "import chardet\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "from scipy import spatial\n",
        "from scipy.stats import pearsonr\n",
        "from utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder, RobustScaler, LabelEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "HOykW3rNHEqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMU-iOMu8r3L"
      },
      "source": [
        "### Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyz2I7OT7LrL"
      },
      "outputs": [],
      "source": [
        "# Function for reading in Stevenson, 2020 data files and uniting rater_01 and rater_02 files\n",
        "def read_files(path):\n",
        "    \"\"\"\n",
        "    Read the csv files from ./data/Stevenson-2020-human\n",
        "\n",
        "    :param path: string with path to files\n",
        "    :return dataset: merged dataset\n",
        "    \"\"\"\n",
        "\n",
        "    path = path\n",
        "    all_files = glob.glob(path + \"/*.csv\")\n",
        "    liR1 = []\n",
        "    liR2 = []\n",
        "\n",
        "    for filename in all_files:\n",
        "        df = pd.read_csv(filename, index_col=None, header=0, nrows=1, encoding='latin1')\n",
        "        if len(df.columns) == 1:\n",
        "            df = pd.read_csv(filename, index_col=None, header=0, encoding='latin1', sep=';')\n",
        "        else:\n",
        "            df = pd.read_csv(filename, index_col=None, header=0, encoding='latin1')\n",
        "\n",
        "        if '_rater01' in filename:\n",
        "            liR1.append(df)\n",
        "\n",
        "\n",
        "        else:\n",
        "            liR2.append(df.loc[:, ['response_id', 'respondent_id', 'originality_rater02', 'utility_rater02']])\n",
        "\n",
        "    frameR1 = pd.concat(liR1, axis=0, ignore_index=True)\n",
        "    frameR2 = pd.concat(liR2, axis=0, ignore_index=True)\n",
        "\n",
        "    df = frameR1.merge(frameR2, on=['response_id','respondent_id'],\n",
        "                   how='left')\n",
        "\n",
        "    df[\"translated_response\"] = df[\"translated_response\"].astype(str)\n",
        "    df[\"response_id\"] = df[\"response_id\"].astype(str)\n",
        "    df[\"respondent_id\"] = df[\"respondent_id\"].astype(str)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in files where encoding needs to be detected (change file paths as needed)\n",
        "with open('/data/Nath-2024-LLM.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "df1 = pd.read_csv('/data/Nath-2024-LLM.csv', encoding=result['encoding'])\n",
        "\n",
        "with open('/data/Hubert-2024-LLM.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "df2 = pd.read_csv('/data/Hubert-2024-LLM.csv', encoding=result['encoding'])"
      ],
      "metadata": {
        "id": "XZb234gdG694"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-42ruafVA3G4"
      },
      "outputs": [],
      "source": [
        "# Loading the rest of the data files (change file paths as needed)\n",
        "df3 = pd.read_csv('/data/Nath-2024-human.csv')\n",
        "df4 = pd.read_csv('/data/Stevenson-2022-LLM.csv')\n",
        "df5 = read_files('/data/Stevenson-2020-human')\n",
        "df6 = pd.read_excel('/data/additional-LLM.xlsx')\n",
        "\n",
        "# Uniting all data files in one dataframe\n",
        "df = pd.concat([df1, df2, df3, df4, df5, df6], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pq3cCBSV5Ft"
      },
      "outputs": [],
      "source": [
        "# Crowdsourcing adjectives from invalid answers to AUT\n",
        "df['translated_response'] = df['translated_response'].astype(str)\n",
        "\n",
        "# Ensuring they are adjectives and at least 3 characters\n",
        "def is_adjective_and_long_enough(word):\n",
        "    return len(word) >= 3 and len(wn.synsets(word, pos=wn.ADJ)) > 0\n",
        "\n",
        "filtered_df = df[\n",
        "    (df[['utility_rater01', 'utility_rater02', 'originality_rater01', 'originality_rater02']] == 0).any(axis=1) &\n",
        "    df['translated_response'].str.split().apply(lambda x: len(x) == 1 and is_adjective_and_long_enough(x[0]))\n",
        "]\n",
        "\n",
        "# Taking 4 most popular\n",
        "properties = filtered_df.groupby('object')['translated_response'].apply(\n",
        "    lambda x: x.value_counts().index.tolist()[:4]\n",
        ").to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akif1HKYVbzl"
      },
      "source": [
        "## Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YysP9E87Y-lj"
      },
      "outputs": [],
      "source": [
        "# Function for dropping invalid answers\n",
        "def drop_invalid(df):\n",
        "    \"\"\"\n",
        "    Drops all answers that were either empty, had a rating of 0 for at least one score,\n",
        "    or of which the respondent number was 9999 (indicating an invalid respondent)\n",
        "\n",
        "    :param df: dataset with all columns needed for further steps\n",
        "    :return dataset, dropped_data:  dataset without invalid data,\n",
        "                dataset of invalid data\n",
        "    \"\"\"\n",
        "    liV = [1] * len(df)\n",
        "    condition = (df[['utility_rater01', 'utility_rater02', 'originality_rater01', 'originality_rater02']] == 0).any(axis=1)\n",
        "\n",
        "    liV = [0 if cond else li for cond, li in zip(condition, liV)]\n",
        "\n",
        "    # Dropping answers rated as 0 by at least one rater\n",
        "    df['valid'] = liV\n",
        "    df_invalid = df[df['valid'] == 0]\n",
        "    df = df[df['valid'] != 0]\n",
        "\n",
        "    # Dropping respondent_id that seems to belong to no one\n",
        "    df_strange = df[df['respondent_id'] == 9999]\n",
        "    df = df[df['respondent_id'] != 9999]\n",
        "\n",
        "    # Dropping empty answers\n",
        "    df_empty = df[df['original_response'] == 'nan']\n",
        "    df = df[df['original_response'] != 'nan']\n",
        "    df = df.drop(columns=['valid'])\n",
        "\n",
        "    df_dropped = pd.concat([df_empty, df_strange, df_invalid], axis=0, ignore_index=True)\n",
        "\n",
        "    return df, df_dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOqvD0dpagW6"
      },
      "outputs": [],
      "source": [
        "# Applying to dataset\n",
        "df, df_dropped = drop_invalid(df)\n",
        "num_dropped = len(df_dropped)\n",
        "\n",
        "print(num_dropped)\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B16lbKj0iImX"
      },
      "outputs": [],
      "source": [
        "# Function for cleaning valid responses\n",
        "def clean_response(dataset, col_response):\n",
        "    \"\"\"\n",
        "    Function cleans the responses\n",
        "\n",
        "    :param dataset: dataset which include column(s) of responses\n",
        "    :param col_response: column name of responses to be cleaned\n",
        "    :return dataset: input dataset with clean responses added\n",
        "    \"\"\"\n",
        "    # Upper to lowercase, remove punctuation and redundant spaces/letters\n",
        "    dataset[col_response] = [x.lower() for x in dataset[col_response]]\n",
        "    dataset[col_response] = [re.sub(r'[^\\w\\s]', ' ', x) for x in dataset[col_response]]  # delete any signs\n",
        "    dataset[col_response] = [re.sub(r'\\b\\w\\b', ' ', x) for x in dataset[col_response]] # delete loose letters\n",
        "    dataset[col_response] = [x.strip() for x in dataset[col_response]]  # delete extra white space before/after string\n",
        "    dataset[col_response] = [' '.join(x.split()) for x in dataset[col_response]]  # delete every extra space in string\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiaI5HysiPUO"
      },
      "outputs": [],
      "source": [
        "# Applying to dataset\n",
        "df = clean_response(df, 'translated_response')\n",
        "df = clean_response(df, 'final_response')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords from each use\n",
        "def remove_stopwords(phrase):\n",
        "    words = word_tokenize(phrase)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "ru4nG7QKTycY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['translated_response'] = df['translated_response'].apply(lambda x: remove_stopwords(x))"
      ],
      "metadata": {
        "id": "g1Ks-HoZTze5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming utility from 5-point scale to three categories\n",
        "df = df.dropna(subset=[\"originality_rater01\", \"utility_rater01\"])\n",
        "\n",
        "# Function to categorize ratings\n",
        "def categorize_rating(rating):\n",
        "    if rating in [1, 2]:\n",
        "        return 'low'\n",
        "    elif rating in [3, 4]:\n",
        "        return 'medium'\n",
        "    elif rating == 5:\n",
        "        return 'high'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "df['category_rater01'] = df['utility_rater01'].apply(categorize_rating)\n",
        "df['category_rater02'] = df['utility_rater02'].apply(lambda x: categorize_rating(x) if not pd.isna(x) else np.nan)\n",
        "\n",
        "# Function that takes rater 1's (better rater's) scores if raters disagree on category\n",
        "def determine_final_category(row):\n",
        "    if pd.isna(row['utility_rater02']):\n",
        "        return row['category_rater01']\n",
        "    elif row['category_rater01'] == row['category_rater02']:\n",
        "        return row['category_rater01']\n",
        "    else:\n",
        "        return row['category_rater02']\n",
        "\n",
        "df['final_category'] = df.apply(determine_final_category, axis=1)\n",
        "\n",
        "# Drop intermediate categories\n",
        "df = df.drop(columns=['category_rater01', 'category_rater02'])"
      ],
      "metadata": {
        "id": "t8vJgo-etc-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL5LaHHJWfSu"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jXbGe15Xf2c"
      },
      "source": [
        "### Elaboration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az-Sr_7KjM71"
      },
      "outputs": [],
      "source": [
        "# Function for counting number of words in response (elaboration)\n",
        "def num_words(dataset, col_response):\n",
        "    \"\"\"\n",
        "    Function to calculate the number of words of each response\n",
        "\n",
        "    :param dataset: dataset containing the columns of col_responses\n",
        "    :param col_response: column name of responses of which words are counted\n",
        "    :return dataset: input dataset with number of words\n",
        "    \"\"\"\n",
        "    answers = dataset[col_response].copy()\n",
        "    numWords = [len(x.split()) for x in answers]\n",
        "    dataset[\"number_words\"] = numWords.copy()\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exr3ICBpjN7W"
      },
      "outputs": [],
      "source": [
        "# Applying to dataset\n",
        "df = num_words(df, 'translated_response')\n",
        "\n",
        "# Descriptive statistics of elaboration\n",
        "mean_words = df.number_words.mean().round(1)\n",
        "sd_words = df.number_words.std().round(1)\n",
        "print(mean_words)\n",
        "print(sd_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ociyMo5uXik3"
      },
      "source": [
        "### (Inverse) Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCqZ1Z5xjbW1"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the frequency and inverse frequency of a response\n",
        "def frequency(dataset, col_response):\n",
        "    \"\"\"\n",
        "    Function to calculate the (inverse) frequency of each response in the data given the object\n",
        "\n",
        "    :param dataset: dataset containing col_obj and col_responses\n",
        "    :param col_response: column name of the responses\n",
        "    :return dataset: input dataset including the inverse frequency and frequency\n",
        "    \"\"\"\n",
        "    # Get unique set of objects\n",
        "    objects = list(set(dataset['object']))\n",
        "\n",
        "    df_objects = []\n",
        "    top_10s = {}\n",
        "    for obj in objects:\n",
        "        # All answers for one object\n",
        "        df_obj = dataset[dataset['object'] == obj].copy()\n",
        "\n",
        "        # Frequency of each answer within object\n",
        "        frequency_answers = pd.DataFrame(df_obj.translated_response.value_counts())\n",
        "        # Creates dictionary of 10 most frequent answers for each object\n",
        "        top_10s[f\"{obj}\"] = df_obj.translated_response.value_counts().head(10).index.to_list()\n",
        "        df_obj['frequency_answer'] = 0\n",
        "        df_obj['frequency_answer_inverse'] = 0\n",
        "\n",
        "        # Add (inverse) frequency of each response to data\n",
        "        for resp, freq in frequency_answers.itertuples():\n",
        "            ind = df_obj.index[df_obj[col_response] == resp].tolist()\n",
        "            df_obj.loc[ind, 'frequency_answer'] = freq\n",
        "            df_obj.loc[ind, 'frequency_answer_inverse'] = 1 / freq\n",
        "        df_objects.append(df_obj)\n",
        "\n",
        "    dataset = pd.concat(df_objects, axis=0, ignore_index=True)\n",
        "\n",
        "    return dataset, top_10s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xofABu1Ejqc-"
      },
      "outputs": [],
      "source": [
        "# Add frequency of each response to df + produce dictionary of top uses\n",
        "df, top_10s = frequency(df, 'translated_response')\n",
        "\n",
        "# Descriptive statistics of (inverse) frequency\n",
        "mean_frequency = df.frequency_answer.mean().round(1)\n",
        "sd_frequency = df.frequency_answer.std().round(1)\n",
        "mean_inverse = df.frequency_answer_inverse.mean().round(2)\n",
        "sd_inverse = df.frequency_answer_inverse.std().round(2)\n",
        "\n",
        "print(mean_frequency)\n",
        "print(sd_frequency)\n",
        "print(mean_inverse)\n",
        "print(sd_inverse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lloEOEEJXmxd"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading GloVe embeddings\n",
        "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "glove_zip = \"glove.6B.zip\"\n",
        "glove_dir = \"glove.6B\"\n",
        "\n",
        "if not os.path.exists(glove_zip):\n",
        "    print(f\"Downloading {glove_zip}...\")\n",
        "    response = requests.get(glove_url)\n",
        "    with open(glove_zip, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Extracting the embeddings\n",
        "if not os.path.exists(glove_dir):\n",
        "    print(f\"Extracting {glove_zip}...\")\n",
        "    with zipfile.ZipFile(glove_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(glove_dir)"
      ],
      "metadata": {
        "id": "Y6ENAcrsQf-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for loading the GloVe embeddings\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
        "    return embeddings_index\n",
        "\n",
        "glove_file = \"glove.6B/glove.6B.100d.txt\"\n",
        "embeddings_index = load_glove_embeddings(glove_file)"
      ],
      "metadata": {
        "id": "aDFLht1rQi_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for getting the GloVe embedding for each response\n",
        "def get_glove_embeddings(df, embeddings_index, embedding_dim=100):\n",
        "    texts = df['final_response'].to_numpy()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    embeddings = []\n",
        "\n",
        "    for text in texts:\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "        word_embeddings = [embeddings_index[word] for word in words if word in embeddings_index]\n",
        "\n",
        "        if word_embeddings:\n",
        "            sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            sentence_embedding = np.zeros(embedding_dim)\n",
        "\n",
        "        embeddings.append(sentence_embedding)\n",
        "\n",
        "    # Converting embeddings from list to df\n",
        "    embeddings_df = pd.DataFrame(embeddings, columns=[f\"embedding_{i}\" for i in range(embedding_dim)])\n",
        "\n",
        "    # Adding to the original df\n",
        "    df = pd.concat([df.reset_index(drop=True), embeddings_df], axis=1)\n",
        "    return df"
      ],
      "metadata": {
        "id": "dIYl4NGgQm-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying to the dataset\n",
        "df = get_glove_embeddings(df, embeddings_index)"
      ],
      "metadata": {
        "id": "moveQ59NQuj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Distance"
      ],
      "metadata": {
        "id": "fO1bj9zbQvwI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQDxu-P6BUb5"
      },
      "outputs": [],
      "source": [
        "# Getting Universal Sentence Encoder embeddings\n",
        "def get_embeddings(texts, batch_size=100):\n",
        "      # Load USE module\n",
        "      os.environ['TFHUB_CACHE_DIR'] = '/tf_cache'\n",
        "      module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3')\n",
        "      embeddings = []\n",
        "      for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        embeddings.append(module(batch).numpy())\n",
        "\n",
        "      return np.vstack(embeddings)\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "      return 1 - spatial.distance.cosine(vec1, vec2)\n",
        "\n",
        "# Function for calculating semantic distance from USE embeddings\n",
        "def sem_dis(dataset, batch_size=100):\n",
        "    \"\"\"\n",
        "    Function to add word embeddings and calculate semantic distance.\n",
        "\n",
        "    :param dataset: dataset containing at least the responses\n",
        "    :return dataset: input with extra columns for the word embedding values (one column per value)\n",
        "                        and the semantic distance\n",
        "    \"\"\"\n",
        "    # Array containing all original responses\n",
        "    responses = dataset[['translated_response']].copy()\n",
        "    objs = pd.DataFrame([['belt'], ['book'], ['brick'], ['can'], ['fork'], ['paperclip'], ['stick'], ['towel']], columns=['translated_response'])\n",
        "    responses = pd.concat([responses, objs], axis=0, ignore_index=True)\n",
        "    responses = responses.to_numpy()\n",
        "\n",
        "    # Get embeddings for top 10 uses for each object\n",
        "    top_use_embeddings = {obj: get_embeddings(uses) for obj, uses in top_10s.items()}\n",
        "\n",
        "    # Get embeddings for the 'translated_response' column in the dataset\n",
        "    emb = get_embeddings(responses, batch_size=batch_size)\n",
        "    emb = pd.DataFrame(emb)\n",
        "    emb.columns = emb.columns.astype(str)\n",
        "\n",
        "    # Word embeddings of the AUT objects\n",
        "    belt = emb.iloc[-8, :]\n",
        "    book = emb.iloc[-7, :]\n",
        "    brick = emb.iloc[-6, :]\n",
        "    can = emb.iloc[-5, :]\n",
        "    fork = emb.iloc[-4, :]\n",
        "    paperclip = emb.iloc[-3, :]\n",
        "    stick = emb.iloc[-2, :]\n",
        "    towel = emb.iloc[-1, :]\n",
        "\n",
        "    emb = emb.iloc[:-8, :]\n",
        "\n",
        "    dist = []\n",
        "\n",
        "    # Calculate semantic distance from AUT object for each response\n",
        "    for i in range(len(emb)):\n",
        "        if dataset['object'][i] == 'belt':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], belt))\n",
        "        elif dataset['object'][i] == 'book':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], book))\n",
        "        elif dataset['object'][i] == 'brick':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], brick))\n",
        "        elif dataset['object'][i] == 'can':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], can))\n",
        "        elif dataset['object'][i] == 'fork':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], fork))\n",
        "        elif dataset['object'][i] == 'paperclip':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], paperclip))\n",
        "        elif dataset['object'][i] == 'stick':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], stick))\n",
        "        elif dataset['object'][i] == 'towel':\n",
        "            dist.append(spatial.distance.cosine(emb.iloc[i, :], towel))\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "      obj = row['object']\n",
        "      use_embedding = emb.iloc[i, :]\n",
        "\n",
        "      # Calculate similarity between response and top 10 uses for that object\n",
        "      if obj in top_use_embeddings:\n",
        "          similarities = [cosine_similarity(use_embedding, top_use_embedding) for top_use_embedding in top_use_embeddings[obj]]\n",
        "          # Store similarity to each of the top 10 uses in a separate column\n",
        "          for j, sim in enumerate(similarities):\n",
        "              dataset.loc[i, f'sim_{j+1}'] = sim\n",
        "\n",
        "    dataset[\"similarity\"] = dist\n",
        "\n",
        "    return dataset, emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhO4wRP3qUzC"
      },
      "outputs": [],
      "source": [
        "# Adding semantic distance to df + creating embeddings\n",
        "df, embeddings = sem_dis(df, batch_size=100)\n",
        "\n",
        "# Descriptive statistics of semantic distance\n",
        "mean_sem_dis = df.similarity.mean().round(2)\n",
        "sd_sem_dis = df.similarity.std().round(2)\n",
        "print(mean_sem_dis)\n",
        "print(sd_sem_dis)\n",
        "\n",
        "max_sem_dis = df.similarity.max().round(2)\n",
        "max_sem_dis_resp = df['translated_response'][df.similarity.idxmax()]\n",
        "max_sem_dis_obj = df['object'][df.similarity.idxmax()]\n",
        "print(max_sem_dis)\n",
        "print(max_sem_dis_resp)\n",
        "print(max_sem_dis_obj)\n",
        "\n",
        "min_sem_dis = df.similarity.min().round(2)\n",
        "min_sem_dis_resp = df['translated_response'][df.similarity.idxmin()]\n",
        "min_sem_dis_obj = df['object'][df.similarity.idxmin()]\n",
        "print(min_sem_dis)\n",
        "print(min_sem_dis_resp)\n",
        "print(min_sem_dis_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FX-eoDVsZKG"
      },
      "source": [
        "### Knowledge Graph Predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZRuC9GXskiu"
      },
      "source": [
        "#### COMET Top 10 Uses\n",
        "\n",
        "To load in the COMET knowledge graph, you first need to download the download_model.sh file from [here](https://github.com/allenai/comet-atomic-2020/tree/master/models/comet_atomic2020_bart). Then, you need to run the following command in your command line:\n",
        "\n",
        "bash download_model.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-gvDV7_CohO"
      },
      "outputs": [],
      "source": [
        "# Set up for COMET knowledge graph\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "\n",
        "class Comet:\n",
        "    def __init__(self, model_path):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        task = \"summarization\"\n",
        "        use_task_specific_params(self.model, task)\n",
        "        self.batch_size = 1\n",
        "        self.decoder_start_token_id = None\n",
        "\n",
        "    def generate(\n",
        "            self,\n",
        "            queries,\n",
        "            decode_method=\"beam\",\n",
        "            num_generate=5,\n",
        "            ):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            examples = queries\n",
        "\n",
        "            decs = []\n",
        "            for batch in list(chunks(examples, self.batch_size)):\n",
        "\n",
        "                batch = self.tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
        "                input_ids, attention_mask = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
        "\n",
        "                summaries = self.model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_start_token_id=self.decoder_start_token_id,\n",
        "                    num_beams=num_generate,\n",
        "                    num_return_sequences=num_generate,\n",
        "                    )\n",
        "\n",
        "                dec = self.tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "                decs.append(dec)\n",
        "\n",
        "            return decs\n",
        "\n",
        "\n",
        "all_relations = [\n",
        "    \"AtLocation\",\n",
        "    \"CapableOf\",\n",
        "    \"Causes\",\n",
        "    \"CausesDesire\",\n",
        "    \"CreatedBy\",\n",
        "    \"DefinedAs\",\n",
        "    \"DesireOf\",\n",
        "    \"Desires\",\n",
        "    \"HasA\",\n",
        "    \"HasFirstSubevent\",\n",
        "    \"HasLastSubevent\",\n",
        "    \"HasPainCharacter\",\n",
        "    \"HasPainIntensity\",\n",
        "    \"HasPrerequisite\",\n",
        "    \"HasProperty\",\n",
        "    \"HasSubEvent\",\n",
        "    \"HasSubevent\",\n",
        "    \"HinderedBy\",\n",
        "    \"InheritsFrom\",\n",
        "    \"InstanceOf\",\n",
        "    \"IsA\",\n",
        "    \"LocatedNear\",\n",
        "    \"LocationOfAction\",\n",
        "    \"MadeOf\",\n",
        "    \"MadeUpOf\",\n",
        "    \"MotivatedByGoal\",\n",
        "    \"NotCapableOf\",\n",
        "    \"NotDesires\",\n",
        "    \"NotHasA\",\n",
        "    \"NotHasProperty\",\n",
        "    \"NotIsA\",\n",
        "    \"NotMadeOf\",\n",
        "    \"ObjectUse\",\n",
        "    \"PartOf\",\n",
        "    \"ReceivesAction\",\n",
        "    \"RelatedTo\",\n",
        "    \"SymbolOf\",\n",
        "    \"UsedFor\",\n",
        "    \"isAfter\",\n",
        "    \"isBefore\",\n",
        "    \"isFilledBy\",\n",
        "    \"oEffect\",\n",
        "    \"oReact\",\n",
        "    \"oWant\",\n",
        "    \"xAttr\",\n",
        "    \"xEffect\",\n",
        "    \"xIntent\",\n",
        "    \"xNeed\",\n",
        "    \"xReact\",\n",
        "    \"xReason\",\n",
        "    \"xWant\",\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQKHweSnoxDE"
      },
      "outputs": [],
      "source": [
        "# Calculating semantic distance between object and 10 uses generated by COMET\n",
        "\n",
        "# Remove stopwords from each use\n",
        "def remove_stopwords(phrase):\n",
        "    words = word_tokenize(phrase)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Remove uses that are too similar\n",
        "def remove_similar_phrases(key, phrases, threshold=20):\n",
        "    phrases_no_stopwords = [remove_stopwords(phrase) for phrase in phrases]\n",
        "\n",
        "    unique_phrases = []\n",
        "\n",
        "    key_no_stopwords = remove_stopwords(key)\n",
        "    unique_phrases.append(key_no_stopwords)\n",
        "\n",
        "    for i, phrase in enumerate(phrases_no_stopwords):\n",
        "        similarity_scores = [fuzz.ratio(phrase, unique_phrase) for unique_phrase in unique_phrases]\n",
        "        if all(score < threshold for score in similarity_scores):\n",
        "            unique_phrases.append(phrase)\n",
        "\n",
        "    # Ensure there are exactly 10 phrases\n",
        "    filtered_phrases = [phrases[i] for i, phrase in enumerate(phrases_no_stopwords) if remove_stopwords(phrases[i]) in unique_phrases[1:]]\n",
        "    return filtered_phrases[:10]\n",
        "\n",
        "# Use the xUsedFor relationship in COMET to generate uses for each object\n",
        "def generate_and_filter_phrases(objects, threshold=20):\n",
        "    top10_uses = {}\n",
        "    comet = Comet(\"/content/comet-atomic_2020_BART_aaai\")\n",
        "    comet.model.zero_grad()\n",
        "\n",
        "    for obj in objects:\n",
        "        queries = []\n",
        "        head = obj\n",
        "        rel = \"xUsedFor\"\n",
        "        query = \"{} {}\".format(head, rel)\n",
        "        queries.append(query)\n",
        "\n",
        "        # Generate more uses until we have 10 unique ones\n",
        "        filtered_results = []\n",
        "        while len(filtered_results) < 10:\n",
        "            results = comet.generate(queries, decode_method=\"greedy\", num_generate=20)\n",
        "            combined_results = filtered_results + results[0]\n",
        "            filtered_results = remove_similar_phrases(head, combined_results, threshold)\n",
        "\n",
        "        filtered_results = filtered_results[:10]\n",
        "        top10_uses[obj] = [filtered_results]\n",
        "\n",
        "    return top10_uses\n",
        "\n",
        "def calculate_similarities(dataset, top10_uses, emb):\n",
        "    top_use_embeddings = {}\n",
        "\n",
        "    # Calculate embeddings for 10 uses from COMET\n",
        "    top_use_embeddings = {obj: get_embeddings(uses) for obj, uses in top10_uses.items()}\n",
        "\n",
        "    dist = []\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "        obj = row['object']\n",
        "        use_embedding = emb.iloc[i, :].values\n",
        "\n",
        "        # Calculate semantic distance between response and each of 10 uses for that object\n",
        "        if obj in top_use_embeddings:\n",
        "            similarities = [cosine_similarity(use_embedding, top_use_embedding) for top_use_embedding in top_use_embeddings[obj]]\n",
        "            # Store similarity for each use in a separate column\n",
        "            for j, sim in enumerate(similarities):\n",
        "                dataset.loc[i, f'comet_sim_{j+1}'] = sim\n",
        "\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Apply for AUT objects in dataset\n",
        "    objects = ['belt', 'book', 'brick', 'can', 'fork', 'paperclip', 'stick', 'towel']\n",
        "    filtered_dict = generate_and_filter_phrases(objects, threshold=70)\n",
        "    updated_dataset = calculate_similarities(df, filtered_dict, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjZ4N7tNsrK_"
      },
      "source": [
        "#### Related Uses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vXjMNCzE-kF"
      },
      "outputs": [],
      "source": [
        "# Generating properties for AUT objects (which don't already have them from invalid AUT responses) using LLM to then be able to generate related uses based on these\n",
        "\n",
        "# List of objects that don't have adjectives from invalid AUT responses\n",
        "objects = ['belt', 'book', 'stick']\n",
        "\n",
        "objects_with_properties = {}\n",
        "\n",
        "# Headers for the API request from Together.ai\n",
        "url = \"https://api.together.xyz/v1/chat/completions\"\n",
        "api_key = # Insert API key\n",
        "headers = {\n",
        "    \"accept\": \"application/json\",\n",
        "    \"content-type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "}\n",
        "\n",
        "# Function to get 4 characteristics for each object from Llama-3\n",
        "def get_characteristics(obj):\n",
        "    payload = {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"What are the top 4 characteristics of a {obj}? I would like simply a list of 4 adjectives describing the object.\"\n",
        "            }\n",
        "        ],\n",
        "        \"model\": \"meta-llama/Llama-3-8b-chat-hf\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        characteristics_text = result['choices'][0]['message']['content']\n",
        "        # Extract characteristics from the response text\n",
        "        characteristics = re.findall(r'\\d+\\.\\s+([\\w\\-]+)', characteristics_text)\n",
        "        return characteristics\n",
        "    else:\n",
        "        print(f\"Failed to get characteristics for {obj}: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Get each object's characteristics\n",
        "for obj in objects:\n",
        "    characteristics = get_characteristics(obj)\n",
        "    objects_with_properties[obj] = characteristics\n",
        "    time.sleep(0.7)\n",
        "\n",
        "print(objects_with_properties)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxtSCYzmcfzg"
      },
      "outputs": [],
      "source": [
        "# Merge with previously gathered characteristics (from invalid AUT responses)\n",
        "objects_with_properties.update(properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11-78ow2Kyuh"
      },
      "outputs": [],
      "source": [
        "# Generating related uses using COMET\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to remove stopwords and lemmatize\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Function to remove similar phrases\n",
        "def remove_similar_phrases(phrases, threshold=70):\n",
        "    unique_phrases = []\n",
        "    for phrase in phrases:\n",
        "        if all(fuzz.ratio(phrase, unique_phrase) < threshold for unique_phrase in unique_phrases):\n",
        "            unique_phrases.append(phrase)\n",
        "    return unique_phrases\n",
        "\n",
        "# Querying COMET for related uses for previously established characteristics using RelatedTo relationship\n",
        "def generate_and_filter_phrases(objects_with_characteristics, threshold=20):\n",
        "    top5_uses = {}\n",
        "    comet = Comet(\"/content/comet-atomic_2020_BART_aaai\")\n",
        "    comet.model.zero_grad()\n",
        "\n",
        "    for obj, characteristics in objects_with_characteristics.items():\n",
        "        all_results = []\n",
        "\n",
        "        for characteristic in characteristics:\n",
        "            queries = []\n",
        "            start = characteristic\n",
        "            rel = \"RelatedTo\"\n",
        "            query = \"{} {}\".format(start, rel)\n",
        "            queries.append(query)\n",
        "            iteration_count = 0\n",
        "            filtered_results = []\n",
        "            num_generate = 20\n",
        "\n",
        "            # Generate more related concepts until we have 5 unique ones\n",
        "            while len(filtered_results) < 5 and iteration_count < 1:\n",
        "                iteration_count += 1\n",
        "                results = comet.generate(queries, decode_method=\"greedy\", num_generate=num_generate)[0]\n",
        "                processed_results = [preprocess_text(result) for result in results]\n",
        "                combined_results = filtered_results + processed_results\n",
        "                filtered_results = remove_similar_phrases(combined_results, threshold)\n",
        "\n",
        "            # Ensure exactly 5 unique concepts for each characteristic\n",
        "            filtered_results = filtered_results[:5]\n",
        "            all_results.extend(filtered_results)\n",
        "\n",
        "        # Remove similar concepts\n",
        "        all_results = remove_similar_phrases(all_results, 80)\n",
        "        top5_uses[obj] = all_results[:8]\n",
        "\n",
        "    return top5_uses\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Applying to previously generated characteristics of AUT objects\n",
        "    top5_uses = generate_and_filter_phrases(objects_with_properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUXYqdOm15ff"
      },
      "outputs": [],
      "source": [
        "# Calculating semantic distance between response and the generated related uses\n",
        "\n",
        "def calculate_similarities(dataset, final_results, emb):\n",
        "    # Generate embeddings for related uses\n",
        "    top_use_embeddings = {obj: get_embeddings(uses) for obj, uses in final_results.items()}\n",
        "\n",
        "    dist = []\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "        obj = row['object']\n",
        "        use_embedding = emb.iloc[i, :].values\n",
        "\n",
        "        # Calculating similarity between response and each related use for that object\n",
        "        if obj in top_use_embeddings and top_use_embeddings[obj] is not None:\n",
        "          similarities = [cosine_similarity(use_embedding, top_use_embedding) for top_use_embedding in top_use_embeddings[obj]]\n",
        "          # Store similarity for each of the 8 related uses in a separate column\n",
        "          for j, sim in enumerate(similarities):\n",
        "            dataset.loc[i, f'related_obj_sim_{j+1}'] = sim\n",
        "        else:\n",
        "          print(f\"Nothing for {obj}.\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Applying to dataset\n",
        "df = calculate_similarities(df, top5_uses, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur4G15MqWlzc"
      },
      "source": [
        "## Cleaning Up Predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0dPAH8js-RX"
      },
      "outputs": [],
      "source": [
        "# Function to identify low variance predictors in the feature set\n",
        "def low_var(dataset, variance_threshold):\n",
        "    \"\"\"\n",
        "    function to remove features with low variance\n",
        "\n",
        "    :param dataset: dataset containing at least the features\n",
        "    :param variance_threshold: minimum proportion of data that should vary\n",
        "    :return: columns of which  have the same number\n",
        "    \"\"\"\n",
        "    # Define feature set\n",
        "    i1 = dataset.columns.get_loc('final_category') + 1\n",
        "    i2 = dataset.columns.get_loc('related_obj_sim_8') + 1\n",
        "\n",
        "    df_features = dataset.iloc[:, i1:i2].copy()\n",
        "\n",
        "    column_names = df_features.columns.values.tolist()\n",
        "    ord_enc = OrdinalEncoder()\n",
        "    df_features[column_names] = ord_enc.fit_transform(df_features[column_names])\n",
        "\n",
        "    # Compare to variance threshold\n",
        "    selector = VarianceThreshold(threshold=variance_threshold)\n",
        "    selector.fit(df_features)\n",
        "\n",
        "    # Get list of low variance columns\n",
        "    low_var_cols = [column for column in df_features.columns\n",
        "                    if column not in df_features.columns[selector.get_support()]]\n",
        "\n",
        "    return low_var_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS_awvditgai"
      },
      "outputs": [],
      "source": [
        "# Function to transform list into string (for listing dropped columns)\n",
        "def list_to_string(listed_words):\n",
        "    \"\"\"\n",
        "    Functions that makes a written summation of values in a list\n",
        "    :param listed_words: list containing multiple strings\n",
        "    :return: one string with the items of the input as written summation\n",
        "    \"\"\"\n",
        "    listed_words = [str(x) for x in listed_words]\n",
        "\n",
        "    written_summation = \"\"\n",
        "    for i in range(len(listed_words)):\n",
        "        if i == 0:\n",
        "            written_summation = written_summation + listed_words[i]\n",
        "        elif i == (len(listed_words) - 1):\n",
        "            written_summation = written_summation + \", and \" + listed_words[i]\n",
        "        else:\n",
        "            written_summation = written_summation + \", \" + listed_words[i]\n",
        "\n",
        "    return written_summation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53aoirL1tuj3"
      },
      "outputs": [],
      "source": [
        "# Applying to dataset\n",
        "low_var_cols = low_var(df, 0.3)\n",
        "\n",
        "dropped_features_var = [re.sub('type_', '', x) for x in low_var_cols]\n",
        "dropped_features_var = list_to_string(low_var_cols)\n",
        "print(dropped_features_var)\n",
        "\n",
        "# Dropping low variance columns\n",
        "df = df.drop(low_var_cols, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtFeQGW3tvtL"
      },
      "outputs": [],
      "source": [
        "# Function for removing highly correlated predictors in the feature set\n",
        "def high_cor(dataset, correlation_threshold):\n",
        "    \"\"\"\n",
        "    function to remove features with high correlation\n",
        "\n",
        "    :param dataset: dataset containing at least the features\n",
        "    :param correlation_threshold: minimum correlation for which features should be dropped\n",
        "    :return: dataframe containing features that have a correlation higher than the threshold\n",
        "    \"\"\"\n",
        "    # Define feature set\n",
        "    i1 = dataset.columns.get_loc('final_category') + 1\n",
        "    i2 = dataset.columns.get_loc('related_obj_sim_8') + 1\n",
        "    df_features = dataset.iloc[:, i1:i2]\n",
        "\n",
        "    # Get correlation matrix of features\n",
        "    cor_matrix = df_features.corr().abs()\n",
        "    upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool_))\n",
        "\n",
        "    # Compare to correlation threshold\n",
        "    cor_features = np.where(upper_tri > correlation_threshold)\n",
        "\n",
        "    # Get list of highly correlated predictors\n",
        "    cor_tri = upper_tri.iloc[list(np.unique(cor_features[0])), list(np.unique(cor_features[1]))]\n",
        "\n",
        "    return cor_tri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opIRQF00t4N-"
      },
      "outputs": [],
      "source": [
        "# Applying to dataset\n",
        "high_cor_cols = high_cor(df, .8)\n",
        "\n",
        "dropped_features_cor = list_to_string(high_cor_cols)\n",
        "print(dropped_features_cor)\n",
        "\n",
        "# Drop highly correlated predictors\n",
        "df = df.drop(high_cor_cols, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_quhhKLWrYZ"
      },
      "source": [
        "## Training Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "287YV9tvpacx"
      },
      "outputs": [],
      "source": [
        "# Creating training + validation and test sets\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Separate out the 'towel' responses (which is the hold-out AUT object)\n",
        "towel_responses = df[df['object'] == 'towel']\n",
        "non_towel_responses = df[df['object'] != 'towel']\n",
        "\n",
        "# Perform stratified sampling on the non-towel responses\n",
        "train_non_towel, test_non_towel = train_test_split(\n",
        "    non_towel_responses,\n",
        "    test_size=0.1,\n",
        "    stratify=non_towel_responses['final_category'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Form the final test set ('towel' responses + stratified sample of remaining)\n",
        "test_set = pd.concat([towel_responses, test_non_towel])\n",
        "\n",
        "# Form the final training + validation set\n",
        "train_set = non_towel_responses[~non_towel_responses.index.isin(test_non_towel.index)]\n",
        "train_set = pd.concat([train_set, train_non_towel])\n",
        "\n",
        "# Shuffle final training and test sets\n",
        "train_set = train_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_set = test_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Drop duplicates based on 'translated_response', keeping the first occurrence\n",
        "train_set = train_set.groupby('object', group_keys=False).apply(lambda x: x.drop_duplicates(subset='translated_response', keep='first'))\n",
        "\n",
        "# Reset the index\n",
        "train_set = train_set.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up for training Naive Bayes, Logistic Regression, k-nearest Neighbors and LightGBM models\n",
        "\n",
        "def get_feature_set(dataset):\n",
        "    # Adjust based on which features you want to train models on\n",
        "    i1 = dataset.columns.get_loc('final_category') + 1\n",
        "    i2 = dataset.columns.get_loc('similarity') + 1\n",
        "    feature_set = dataset.iloc[:, i1:i2]\n",
        "\n",
        "# Function for calculating AIC and BIC for model comparison\n",
        "def calculate_aic_bic(log_likelihood, n_params, n_samples):\n",
        "    aic = 2 * n_params - 2 * log_likelihood\n",
        "    bic = np.log(n_samples) * n_params - 2 * log_likelihood\n",
        "    return aic, bic\n",
        "\n",
        "def train_models(train_set, test_set, print_results=False):\n",
        "    SEED = 1\n",
        "    i = 0\n",
        "\n",
        "    # Transforming categorical labels into numbers for model\n",
        "    le = LabelEncoder()\n",
        "    train_set['final_category_encoded'] = le.fit_transform(train_set['final_category'])\n",
        "    test_set['final_category_encoded'] = le.transform(test_set['final_category'])\n",
        "\n",
        "    # Getting feature set and outcome variable\n",
        "    predictorVar = get_feature_set(train_set)\n",
        "    targetVar = 'final_category_encoded'\n",
        "    X_trainval = train_set[predictorVar.columns]\n",
        "    y_trainval = train_set[targetVar]\n",
        "    X_test = test_set[predictorVar.columns]\n",
        "    y_test = test_set[targetVar]\n",
        "\n",
        "    # Setting up 5-fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setting up hyperparameters for each model (adjust as needed)\n",
        "    params_log_reg = {\n",
        "        'C': 1.0,\n",
        "        'penalty': 'l2',\n",
        "        'solver': 'liblinear',\n",
        "        'random_state': SEED\n",
        "    }\n",
        "\n",
        "    params_knn = {\n",
        "        'n_neighbors': 5,\n",
        "        'weights': 'uniform',\n",
        "        'algorithm': 'auto'\n",
        "    }\n",
        "\n",
        "    params_lgb = {\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 3,\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.1,\n",
        "        'n_estimators': 100,\n",
        "        'random_state': SEED\n",
        "    }\n",
        "\n",
        "    log_reg = LogisticRegression(**params_log_reg)\n",
        "    nb = GaussianNB()\n",
        "    knn = KNeighborsClassifier(**params_knn)\n",
        "    lgb_model = lgb.LGBMClassifier(**params_lgb)\n",
        "\n",
        "    # Set up saving for trained models\n",
        "    trained_models = {\"utility\": dict()}\n",
        "    df_predictions = pd.DataFrame(columns=['measure', 'model', 'set', 'actual', 'predictions', 'accuracy', 'precision', 'recall', 'f1_score', 'aic', 'bic'], index=range(0, 14))\n",
        "\n",
        "    models = [('Logistic Regression', log_reg), ('Naive Bayes', nb), ('KNN', knn), ('LightGBM', lgb_model), ('Base Model', 'mode')]\n",
        "\n",
        "    accuracy_test_scores = []\n",
        "    accuracy_val_scores = []\n",
        "    precision_test_scores = []\n",
        "    precision_val_scores = []\n",
        "    recall_test_scores = []\n",
        "    recall_val_scores = []\n",
        "    f1_test_scores = []\n",
        "    f1_val_scores = []\n",
        "    aic_test_scores = []\n",
        "    aic_val_scores = []\n",
        "    bic_test_scores = []\n",
        "    bic_val_scores = []\n",
        "\n",
        "   # Training and validating each model using 5-fold cross-validation\n",
        "    for ml_name, ml in models:\n",
        "        for train_index, val_index in skf.split(X_trainval, y_trainval):\n",
        "            X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
        "            y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
        "\n",
        "            if ml_name == 'Base Model':\n",
        "                mode_val = train_set['final_category_encoded'].mode()[0]\n",
        "                y_pred_val = np.repeat(mode_val, len(X_val))\n",
        "                y_pred_test = np.repeat(mode_val, len(X_test))\n",
        "                y_pred_prob_val = np.ones((len(X_val), len(le.classes_))) / len(le.classes_)\n",
        "                y_pred_prob_test = np.ones((len(X_test), len(le.classes_))) / len(le.classes_)\n",
        "            else:\n",
        "                ml.fit(X_train, y_train)\n",
        "                y_pred_val = ml.predict(X_val)\n",
        "                y_pred_test = ml.predict(X_test)\n",
        "                y_pred_prob_val = ml.predict_proba(X_val)\n",
        "                y_pred_prob_test = ml.predict_proba(X_test)\n",
        "\n",
        "            trained_models['utility'].update({ml_name: ml})\n",
        "\n",
        "            # Calculating accuracy, precision, recall and F1-score\n",
        "            accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "            precision_val = precision_score(y_val, y_pred_val, average='weighted', zero_division=0)\n",
        "            recall_val = recall_score(y_val, y_pred_val, average='weighted')\n",
        "            f1_val = f1_score(y_val, y_pred_val, average='weighted')\n",
        "\n",
        "            accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "            precision_test = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "            recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "            f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "            # Calculating AIC and BIC\n",
        "            log_likelihood_val = -log_loss(y_val, y_pred_prob_val, labels=np.arange(len(le.classes_)))\n",
        "            log_likelihood_test = -log_loss(y_test, y_pred_prob_test, labels=np.arange(len(le.classes_)))\n",
        "            n_params = len(ml.get_params()) if ml_name != 'Base Model' else 0\n",
        "            n_samples_val = len(y_val)\n",
        "            n_samples_test = len(y_test)\n",
        "            aic_val, bic_val = calculate_aic_bic(log_likelihood_val, n_params, n_samples_val)\n",
        "            aic_test, bic_test = calculate_aic_bic(log_likelihood_test, n_params, n_samples_test)\n",
        "\n",
        "            # Appending metrics to appropriate lists\n",
        "            accuracy_test_scores.extend([accuracy_test])\n",
        "            accuracy_val_scores.extend([accuracy_val])\n",
        "            precision_test_scores.extend([precision_test])\n",
        "            precision_val_scores.extend([precision_val])\n",
        "            recall_test_scores.extend([recall_test])\n",
        "            recall_val_scores.extend([recall_val])\n",
        "            f1_test_scores.extend([f1_test])\n",
        "            f1_val_scores.extend([f1_val])\n",
        "            aic_test_scores.extend([aic_test])\n",
        "            aic_val_scores.extend([aic_val])\n",
        "            bic_test_scores.extend([bic_test])\n",
        "            bic_val_scores.extend([bic_val])\n",
        "\n",
        "            # Adding results to df_predictions\n",
        "            df_predictions.loc[i] = np.array(['utility', ml_name, 'test', y_test, y_pred_test, accuracy_test, precision_test, recall_test, f1_test,\n",
        "                                             aic_test, bic_test], dtype=object)\n",
        "\n",
        "            df_predictions.loc[i + 1] = np.array(['utility', ml_name, 'validation', y_val, y_pred_val, accuracy_val, precision_val, recall_val, f1_val,\n",
        "                                                 aic_val, bic_val], dtype=object)\n",
        "\n",
        "            i += 2\n",
        "\n",
        "        # Calculating mean metrics across folds\n",
        "        mean_accuracy_test = np.mean(accuracy_test_scores)\n",
        "        mean_accuracy_val = np.mean(accuracy_val_scores)\n",
        "        mean_precision_test = np.mean(precision_test_scores)\n",
        "        mean_precision_val = np.mean(precision_val_scores)\n",
        "        mean_recall_test = np.mean(recall_test_scores)\n",
        "        mean_recall_val = np.mean(recall_val_scores)\n",
        "        mean_f1_test = np.mean(f1_test_scores)\n",
        "        mean_f1_val = np.mean(f1_val_scores)\n",
        "        mean_aic_test = np.mean(aic_test_scores)\n",
        "        mean_aic_val = np.mean(aic_val_scores)\n",
        "        mean_bic_test = np.mean(bic_test_scores)\n",
        "        mean_bic_val = np.mean(bic_val_scores)\n",
        "\n",
        "        # Printing metrics\n",
        "        if print_results:\n",
        "            print(y_pred_test)\n",
        "            print(\"utility \\n\")\n",
        "            print(\"{:s} \\n Accuracy: {:.3f}, \\n Precision: {:.3f}, \\n Recall: {:.3f} \\n F1: {:.3f}, AIC: {:.3f}, BIC: {:.3f} \\n\".format(ml_name, mean_accuracy_test, mean_precision_test, mean_recall_test, mean_f1_test, mean_aic_test, mean_bic_test))\n",
        "            print(\"\\n\")\n",
        "            print(\"{:s} \\n Accuracy: {:.3f}, \\n Precision: {:.3f}, \\n Recall: {:.3f} \\n F1: {:.3f}, AIC: {:.3f}, BIC: {:.3f} \\n\".format('test set', mean_accuracy_val, mean_precision_val, mean_recall_val, mean_f1_val, mean_aic_val, mean_bic_val))\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return df_predictions, trained_models"
      ],
      "metadata": {
        "id": "k7VaEthJBbym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up for training random forest classifier and XGBoost models\n",
        "\n",
        "def get_feature_set(dataset):\n",
        "    # Adjust based on which features you want to train models on\n",
        "    i1 = dataset.columns.get_loc('final_category') + 1\n",
        "    i2 = dataset.columns.get_loc('similarity') + 1\n",
        "    feature_set = dataset.iloc[:, i1:i2]\n",
        "    return feature_set\n",
        "\n",
        "# Function for calculating AIC and BIC for model comparison\n",
        "def calculate_aic_bic(log_likelihood, n_params, n_samples):\n",
        "    aic = 2 * n_params - 2 * log_likelihood\n",
        "    bic = np.log(n_samples) * n_params - 2 * log_likelihood\n",
        "    return aic, bic\n",
        "\n",
        "def train_models(train_set, test_set, print_results=False):\n",
        "    SEED = 1\n",
        "    i = 0\n",
        "\n",
        "    # Transforming categorical labels into numbers for model\n",
        "    le = LabelEncoder()\n",
        "    train_set['final_category_encoded'] = le.fit_transform(train_set['final_category'])\n",
        "    test_set['final_category_encoded'] = le.transform(test_set['final_category'])\n",
        "\n",
        "    # Getting feature set and outcome variable\n",
        "    predictorVar = get_feature_set(train_set)\n",
        "    targetVar = 'final_category_encoded'\n",
        "    X_trainval = train_set[predictorVar.columns]\n",
        "    y_trainval = train_set[targetVar]\n",
        "    X_test = test_set[predictorVar.columns]\n",
        "    y_test = test_set[targetVar]\n",
        "\n",
        "    # Setting up 5-fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Setting up hyperparameters for each model (adjust as needed)\n",
        "    params_rf = {\n",
        "        'max_depth': 20,\n",
        "        'min_samples_leaf': 3,\n",
        "        'min_samples_split': 8,\n",
        "        'n_estimators': 200,\n",
        "        'class_weight': 'balanced'\n",
        "    }\n",
        "\n",
        "    params_xgb = {\n",
        "        'learning_rate': 0.2,\n",
        "        'max_depth': 10,\n",
        "        'n_estimators': 200,\n",
        "        'subsample': 0.6,\n",
        "        'eta': 0.1,\n",
        "        'objective': 'multi:softmax',  # for classification\n",
        "        'num_class': 3,\n",
        "        'scale_pos_weight': len(y_trainval) / (2 * np.bincount(y_trainval))\n",
        "    }\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=SEED)\n",
        "    rf.set_params(**params_rf)\n",
        "    xgb_model = xgb.XGBClassifier(**params_xgb)\n",
        "\n",
        "    # Set up saving for trained models\n",
        "    trained_models = {\"utility\": dict()}\n",
        "    df_predictions = pd.DataFrame(columns=['measure', 'model', 'set', 'actual', 'predictions', 'accuracy', 'precision', 'recall', 'f1_score', 'aic', 'bic'], index=range(0, 14))\n",
        "\n",
        "    models = [('Random Forest', rf), ('XGBoost', xgb_model), ('Base Model', 'mode')]\n",
        "    accuracy_test_scores = []\n",
        "    accuracy_val_scores = []\n",
        "    precision_test_scores = []\n",
        "    precision_val_scores = []\n",
        "    recall_test_scores = []\n",
        "    recall_val_scores = []\n",
        "    f1_test_scores = []\n",
        "    f1_val_scores = []\n",
        "    aic_test_scores = []\n",
        "    aic_val_scores = []\n",
        "    bic_test_scores = []\n",
        "    bic_val_scores = []\n",
        "\n",
        "    # Training and validating each model using 5-fold cross-validation\n",
        "    for ml_name, ml in models:\n",
        "      for train_index, val_index in skf.split(X_trainval, y_trainval):\n",
        "          X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
        "          y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
        "\n",
        "          if ml_name == 'Random Forest':\n",
        "              ml.fit(X_train, y_train)\n",
        "              y_pred_val = ml.predict(X_val)\n",
        "              y_pred_test = ml.predict(X_test)\n",
        "              y_pred_prob_val = ml.predict_proba(X_val)\n",
        "              y_pred_prob_test = ml.predict_proba(X_test)\n",
        "          elif ml_name == 'Base Model':\n",
        "              mode_val = train_set['final_category_encoded'].mode()[0]\n",
        "              y_pred_val = np.repeat(mode_val, len(X_val))\n",
        "              y_pred_test = np.repeat(mode_val, len(X_test))\n",
        "              y_pred_prob_val = np.ones((len(X_val), len(le.classes_))) / len(le.classes_)\n",
        "              y_pred_prob_test = np.ones((len(X_test), len(le.classes_))) / len(le.classes_)\n",
        "          else:\n",
        "              ml.fit(X_train, y_train)\n",
        "              y_pred_val = ml.predict(X_val)\n",
        "              y_pred_test = ml.predict(X_test)\n",
        "              y_pred_prob_val = ml.predict_proba(X_val)\n",
        "              y_pred_prob_test = ml.predict_proba(X_test)\n",
        "\n",
        "          trained_models['utility'].update({ml_name: ml})\n",
        "\n",
        "          # Calculating accuracy, precision, recall and F1-score\n",
        "          accuracy_val = accuracy_score(y_val, y_pred_val)\n",
        "          precision_val = precision_score(y_val, y_pred_val, average='weighted', zero_division=0)\n",
        "          recall_val = recall_score(y_val, y_pred_val, average='weighted')\n",
        "          f1_val = f1_score(y_val, y_pred_val, average='weighted')\n",
        "\n",
        "          accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "          precision_test = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "          recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "          f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "          # Calculating AIC and BIC\n",
        "          log_likelihood_val = -log_loss(y_val, y_pred_prob_val, labels=np.arange(len(le.classes_)))\n",
        "          log_likelihood_test = -log_loss(y_test, y_pred_prob_test, labels=np.arange(len(le.classes_)))\n",
        "          n_params_rf = len(params_rf) if ml_name == 'Random Forest' else len(params_xgb)\n",
        "          n_samples_val = len(y_val)\n",
        "          n_samples_test = len(y_test)\n",
        "          aic_val, bic_val = calculate_aic_bic(log_likelihood_val, n_params_rf, n_samples_val)\n",
        "          aic_test, bic_test = calculate_aic_bic(log_likelihood_test, n_params_rf, n_samples_test)\n",
        "\n",
        "          # Appending metrics to appropriate lists\n",
        "          accuracy_test_scores.extend([accuracy_test])\n",
        "          accuracy_val_scores.extend([accuracy_val])\n",
        "          precision_test_scores.extend([precision_test])\n",
        "          precision_val_scores.extend([precision_val])\n",
        "          recall_test_scores.extend([recall_test])\n",
        "          recall_val_scores.extend([recall_val])\n",
        "          f1_test_scores.extend([f1_test])\n",
        "          f1_val_scores.extend([f1_val])\n",
        "          aic_test_scores.extend([aic_test])\n",
        "          aic_val_scores.extend([aic_val])\n",
        "          bic_test_scores.extend([bic_test])\n",
        "          bic_val_scores.extend([bic_val])\n",
        "\n",
        "          # Adding results to df_predictions\n",
        "          df_predictions.loc[i] = np.array(['utility', ml_name, 'test', y_test, y_pred_test, accuracy_test, precision_test, recall_test, f1_test,\n",
        "                                             aic_test, bic_test], dtype=object)\n",
        "\n",
        "          df_predictions.loc[i + 1] = np.array(['utility', ml_name, 'validation', y_val, y_pred_val, accuracy_val, precision_val, recall_val, f1_val,\n",
        "                                                 aic_val, bic_val], dtype=object)\n",
        "\n",
        "          i += 2\n",
        "\n",
        "      # Calculating mean metrics across folds\n",
        "      mean_accuracy_test = np.mean(accuracy_test_scores)\n",
        "      mean_accuracy_val = np.mean(accuracy_val_scores)\n",
        "      mean_precision_test = np.mean(precision_test_scores)\n",
        "      mean_precision_val = np.mean(precision_val_scores)\n",
        "      mean_recall_test = np.mean(recall_test_scores)\n",
        "      mean_recall_val = np.mean(recall_val_scores)\n",
        "      mean_f1_test = np.mean(f1_test_scores)\n",
        "      mean_f1_val = np.mean(f1_val_scores)\n",
        "      mean_aic_test = np.mean(aic_test_scores)\n",
        "      mean_aic_val = np.mean(aic_val_scores)\n",
        "      mean_bic_test = np.mean(bic_test_scores)\n",
        "      mean_bic_val = np.mean(bic_val_scores)\n",
        "\n",
        "      # Printing metrics\n",
        "      if print_results:\n",
        "            print(y_pred_test)\n",
        "            print(\"utility \\n\")\n",
        "            print(\"{:s} \\n Accuracy: {:.3f}, \\n Precision: {:.3f}, \\n Recall: {:.3f} \\n F1: {:.3f}, AIC: {:.3f}, BIC: {:.3f} \\n\".format(ml_name, mean_accuracy_test, mean_precision_test, mean_recall_test, mean_f1_test, mean_aic_test, mean_bic_test))\n",
        "            print(\"\\n\")\n",
        "            print(\"{:s} \\n Accuracy: {:.3f}, \\n Precision: {:.3f}, \\n Recall: {:.3f} \\n F1: {:.3f}, AIC: {:.3f}, BIC: {:.3f} \\n\".format('test set', mean_accuracy_val, mean_precision_val, mean_recall_val, mean_f1_val, mean_aic_val, mean_bic_val))\n",
        "            print(\"\\n\")\n",
        "\n",
        "      # Save XGBoost model if ml_name is XGBoost (for feature importance analysis)\n",
        "      if ml_name == 'XGBoost':\n",
        "          joblib.dump(ml, 'xgb_model.pkl')\n",
        "          print(\"XGBoost model saved successfully.\")\n",
        "\n",
        "    return df_predictions, trained_models"
      ],
      "metadata": {
        "id": "hDoxqMqNCNOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models\n",
        "models, df_predictions = train_models(train_set, test_set, print_results = True)"
      ],
      "metadata": {
        "id": "73h3RhPLFWvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmWZtnW4WwZL"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gireIP2f734c"
      },
      "source": [
        "### Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the XGBoost model\n",
        "model = joblib.load('xgb_model.pkl')\n",
        "\n",
        "# Extracting feature importances\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "    feature_importances = model.feature_importances_\n",
        "    feature_names = model.feature_names_in_\n",
        "elif hasattr(model, 'get_score'):\n",
        "    feature_importances = model.get_score(importance_type='weight')\n",
        "    feature_names = list(feature_importances.keys())\n",
        "    feature_importances = list(feature_importances.values())\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort features by importance\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Select top 15 features\n",
        "top_features = importance_df.head(15)\n",
        "\n",
        "# Visualize in a plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(top_features['Feature'], top_features['Importance'], color='#204D74')\n",
        "plt.xlabel('Importance', fontsize = 14)\n",
        "plt.yticks(fontsize = 12)\n",
        "plt.xticks(fontsize = 12)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VShqB9TDtfx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}