{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Evaluation of AUT Response Utility\n",
        "\n",
        "This notebook serves to use a large-language model (Llama-3) via the together.ai API to rate the utility of a test set of responses to the Alternate Uses Task."
      ],
      "metadata": {
        "id": "26E1tr9yAjD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries & Data"
      ],
      "metadata": {
        "id": "Wciv6UuX--P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import together\n",
        "import requests\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "46zZdfOS-j-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-o9tS8xiHY4"
      },
      "outputs": [],
      "source": [
        "# Importing test set\n",
        "test_set = pd.read_excel('/content/test_set2.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Rating From LLM"
      ],
      "metadata": {
        "id": "9W_KTCgH_7wc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ7y-9d0fE5a"
      },
      "outputs": [],
      "source": [
        "# API call to together.ai\n",
        "api_key = # Insert API key\n",
        "def together_call(prompt: str, model: str, temp: float) -> str:\n",
        "    url = \"https://api.together.xyz/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": temp\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=data)\n",
        "        response.raise_for_status()\n",
        "        output = response.json()\n",
        "        return output['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        print(f\"Error during API call: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pck8lBhVhyEY"
      },
      "outputs": [],
      "source": [
        "# Prompt creation\n",
        "def create_prompt(item, response):\n",
        "    return f'''\n",
        "The Alternative Uses Task (AUT) is a task used to measure divergent thinking. Divergent\n",
        "thinking is a process often linked to creativity, where participants must provide as many\n",
        "answers/solutions as possible to a given question. When taking the AUT, a subject is asked to\n",
        "think of as many uses as possible for a given object.\n",
        "\n",
        "Eg. Object: brick\n",
        "Alternative use: doorstop\n",
        "\n",
        "A creative use is defined as one that is both original and useful, where originality\n",
        "outweighs utility. For now, it is important to score utility carefully.\n",
        "\n",
        "Utility\n",
        "\n",
        "The utility of a use is determined by how usable the property is. The utility score is\n",
        "divided into three categories - low, medium and high. The assignment of scores is explained in\n",
        "more detail below:\n",
        "\n",
        "(low) Unusable or Difficult to realize: This score should be assigned to uses that are impossible to\n",
        "realize or difficult to achieve.\n",
        "\n",
        "Eg. Object: Book\n",
        "Alternative usage: Dobber\n",
        "\n",
        "The stated use in this example should be assigned the utility score of low. An essential\n",
        "characteristic of a dobber is that it floats. A book sinks, therefore it is impossible to use a\n",
        "book as a dobber.\n",
        "\n",
        "Eg. Object: Belt\n",
        "Alternative Usage: Fishing Rod\n",
        "\n",
        "The mentioned use in this example should be assigned the utility score low. Indeed, to\n",
        "realize a fishing pole, not only the belt will suffice (which functions as a line), one also\n",
        "needs an additional action/object; in this case, a stick/rod and bait.\n",
        "\n",
        "(medium) Reasonably realizable or Easily realized: This score should be assigned when the use is\n",
        "reasonably realizable or easily achievable. Consider uses that require (very) minor modifications to the\n",
        "named use.\n",
        "\n",
        "Eg. Object: Can\n",
        "Alternative Usage: Camera Tripod\n",
        "\n",
        "The listed use in this example should be assigned the utility score of medium. A can can be\n",
        "used as a tripod but this will require multiple adjustments in some cases. For example, to\n",
        "adjust the height multiple cans will need to be used.\n",
        "\n",
        "Eg. Object: Stick\n",
        "Alternative Usage: Fork\n",
        "\n",
        "The listed use in this example should be assigned the utility score of medium. A stick can be\n",
        "used well as a replacement for a fork. However, in some cases, a stick will need to\n",
        "bemade sharper, but generally, a stick works well as a fork.\n",
        "\n",
        "(high) Always realizable: This score should be assigned to uses that are always\n",
        "realizable. Consider uses that do not require modifications of the named use or uses\n",
        "intended for the given object.\n",
        "\n",
        "Eg. Object: Can\n",
        "Alternative Usage: Pen holder\n",
        "\n",
        "The stated use in this example should be assigned the utility score of high. A tin can can be\n",
        "used as a pen holder without any modifications.\n",
        "\n",
        "You are an expert reviewer. You will be provided with human responses to the AUT task. You need to rate the utility of objects and their alternate\n",
        "uses as described above. You will be given multiple responses together and have to rate each one. Make your best judgement and provide a rating\n",
        "of either low, medium or high for every response. However, you must make sure that you are unbiased and ensure your previous ratings do not impact\n",
        "your later ratings. Format your response such that all the utility ratings are comma separated in the first line.\n",
        "\n",
        "item,response,rating\n",
        "brick, coaster pans, high\n",
        "paperclip, injure, medium\n",
        "book, doorstop, high\n",
        "fork, comb your hair, medium\n",
        "can, picture frame, low\n",
        "belt, pinchers, low\n",
        "\n",
        "item: {item}, response: {response}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq9PjjU-hM9A"
      },
      "outputs": [],
      "source": [
        "# Function for supplementing object and response into prompt and requesting rating\n",
        "def score_dataset(df, model, temp):\n",
        "    ratings = []\n",
        "    for index, row in df.iterrows():\n",
        "        item = row['object']\n",
        "        response = row['translated_response']\n",
        "        prompt = create_prompt(item, response)\n",
        "        rating = together_call(prompt, model, temp)\n",
        "        if rating:\n",
        "            ratings.append(rating.strip())\n",
        "        else:\n",
        "            ratings.append(\"Error\")\n",
        "        time.sleep(0.6) # Pause to not hit rate limit\n",
        "    return ratings\n",
        "\n",
        "# Applying to the test set\n",
        "test_set['rating'] = score_dataset(test_set, \"meta-llama/Llama-3-70b-chat-hf\", 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting rated test set to csv to clean up ratings\n",
        "test_set.to_csv('test_set.csv', index=False)\n",
        "from google.colab import files\n",
        "files.download('test_set.csv')"
      ],
      "metadata": {
        "id": "mgSWbkFxeC1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculation of Evaluation Metrics"
      ],
      "metadata": {
        "id": "Hp6HC3sRACnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing cleaned file\n",
        "test_set = pd.read_csv('/content/test_set(1).csv')\n",
        "\n",
        "# Changing labels to numbers\n",
        "label_encoder = LabelEncoder()\n",
        "test_set['final_category_num'] = label_encoder.fit_transform(test_set['final_category'])\n",
        "test_set['rating_num'] = label_encoder.transform(test_set['rating'])\n",
        "\n",
        "# Calculating evaluation metrics using numerical labels\n",
        "accuracy = accuracy_score(test_set['final_category_num'], test_set['rating_num'])\n",
        "precision = precision_score(test_set['final_category_num'], test_set['rating_num'], average = 'weighted')\n",
        "recall = recall_score(test_set['final_category_num'], test_set['rating_num'], average = 'weighted')\n",
        "f1 = f1_score(test_set['final_category_num'], test_set['rating_num'], average = 'weighted')\n",
        "\n",
        "# Printing results\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "conf_matrix = confusion_matrix(test_set['final_category_num'], test_set['rating_num'])\n",
        "\n",
        "# Plotting confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZtIl_J32_XOK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}